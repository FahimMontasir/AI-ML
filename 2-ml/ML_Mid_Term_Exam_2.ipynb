{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fba7b32d",
      "metadata": {
        "id": "fba7b32d"
      },
      "source": [
        "ML – MIDTERM EXAM (100 Marks)\n",
        "\n",
        "This notebook is your **single submission file** for the Midterm.\n",
        "\n",
        "- **Total marks:** 100  \n",
        "  - Section A: 40 marks  \n",
        "  - Section B: 60 marks  \n",
        "- Answer **all questions** in this notebook.  \n",
        "- Do **not** create a separate PDF.  \n",
        "- Use clear headings, code, and explanations.\n",
        "\n",
        "- Run all cells before submitting so all outputs are visible.\n",
        "- Set the Colab file's shareable link to ‘Anyone with the link’ and ‘View’ access, then submit it in the Phitron Assignment module's Assignment submission section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "2c0a54c6",
      "metadata": {
        "id": "2c0a54c6"
      },
      "outputs": [],
      "source": [
        "# Common imports for Section B (run once)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_curve, auc\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (6, 4)\n",
        "plt.rcParams['axes.grid'] = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f22f0e79",
      "metadata": {
        "id": "f22f0e79"
      },
      "source": [
        "---\n",
        "\n",
        "## SECTION A – Short Application Questions (40 Marks)\n",
        "\n",
        "Write your answers in the provided **answer cells** in this notebook. Use text, formulas, and short reasoning.\n",
        "\n",
        "Marks for each question are clearly mentioned.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "845c66f1",
      "metadata": {
        "id": "845c66f1"
      },
      "source": [
        "### Q1. Descriptive Statistics and Distributions (15 marks)\n",
        "\n",
        "A dataset of monthly customer spending (in dollars) is:\n",
        "\n",
        "`[30, 35, 32, 34, 33, 500, 31, 34, 32, 33]`\n",
        "\n",
        "1. Compute the **median** and **IQR**. Show your working clearly.  \n",
        "2. Use the **IQR rule** to check if 500 is an outlier. Show your steps and the fences.  \n",
        "3. Explain in 3 to 5 sentences why **median + IQR** may be better than **mean + standard deviation** for this dataset.\n",
        "\n",
        "Write your full answer in the cell below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "005abbd0",
      "metadata": {
        "id": "005abbd0"
      },
      "source": [
        "#### Q1 Answer (Student)\n",
        "\n",
        "---\n",
        "\n",
        "## **DIRECT ANSWER:**\n",
        "\n",
        "**Given Data:** [30, 35, 32, 34, 33, 500, 31, 34, 32, 33]\n",
        "\n",
        "### **1. Median and IQR Computation:**\n",
        "- **Sorted data:** [30, 31, 32, 32, 33, 33, 34, 34, 35, 500]\n",
        "- **Median (Q2):** 33.0 (average of 5th and 6th values)\n",
        "- **Q1 (First Quartile):** 32.0\n",
        "- **Q3 (Third Quartile):** 34.0\n",
        "- **IQR (Interquartile Range):** Q3 - Q1 = 34.0 - 32.0 = **2.0**\n",
        "\n",
        "### **2. Outlier Detection Using IQR Rule:**\n",
        "- **Lower fence:** Q1 - 1.5 × IQR = 32 - 1.5(2) = 32 - 3 = **29**\n",
        "- **Upper fence:** Q3 + 1.5 × IQR = 34 + 1.5(2) = 34 + 3 = **37**\n",
        "- **Conclusion:** 500 > 37, therefore **500 is an outlier**\n",
        "\n",
        "### **3. Why Median + IQR is Better Than Mean + SD:**\n",
        "Median and IQR are **robust statistics** that depend on position rather than magnitude, making them resistant to outliers. The extreme value 500 drastically inflates the mean (79.4) and standard deviation (147.8), rendering them unrepresentative of typical customer spending. In contrast, the median (33) accurately reflects the central tendency of the majority, and the IQR (2) shows that the middle 50% of customers have spending concentrated in a narrow range. For this right-skewed dataset with an outlier, median and IQR provide more meaningful and interpretable summaries of customer behavior.\n",
        "\n",
        "---\n",
        "\n",
        "## **QUESTION ANALYSIS:**\n",
        "\n",
        "This question assesses three fundamental statistical concepts:\n",
        "\n",
        "1. **Manual calculation** of resistant measures (median, quartiles, IQR)\n",
        "2. **Application** of Tukey's fences method for outlier detection  \n",
        "3. **Understanding** when robust statistics outperform parametric measures\n",
        "\n",
        "The dataset is deliberately designed with an extreme outlier (500 vs. 30-35) to demonstrate the breakdown of mean/SD and the superiority of median/IQR in skewed distributions.\n",
        "\n",
        "---\n",
        "\n",
        "## **DETAILED ANSWER ANALYSIS WITH THEORY:**\n",
        "\n",
        "### **Part 1: Computing Median and IQR**\n",
        "\n",
        "**Median (Second Quartile):**\n",
        "- The median is the **middle value** that divides sorted data into two equal halves\n",
        "- With n=10 (even count), median = (5th value + 6th value) / 2\n",
        "- Sorted data: [30, 31, 32, 32, **33, 33**, 34, 34, 35, 500]\n",
        "- Median = (33 + 33) / 2 = **33.0**\n",
        "- **Property:** 50% of customers spend ≤ $33\n",
        "\n",
        "**Quartiles:**\n",
        "- **Q1 (25th percentile):** Median of lower half → **32.0**\n",
        "  - Separates lowest 25% from upper 75%\n",
        "- **Q3 (75th percentile):** Median of upper half → **34.0**  \n",
        "  - Separates lowest 75% from upper 25%\n",
        "\n",
        "**IQR (Interquartile Range):**\n",
        "- IQR = Q3 - Q1 = 34 - 32 = **2.0**\n",
        "- Represents the **spread of the middle 50%** of data\n",
        "- **Key property:** Resistant measure—unaffected by extreme values outside the middle 50%\n",
        "\n",
        "### **Part 2: IQR Rule for Outlier Detection**\n",
        "\n",
        "**Tukey's Fences Method (1.5 × IQR Rule):**\n",
        "\n",
        "Standard outlier boundaries:\n",
        "- **Lower fence:** Q1 - 1.5 × IQR = 32 - 1.5(2) = **29**\n",
        "- **Upper fence:** Q3 + 1.5 × IQR = 34 + 1.5(2) = **37**\n",
        "\n",
        "**Decision rule:** Any observation < 29 or > 37 is flagged as an outlier\n",
        "\n",
        "**Application to our data:**\n",
        "- All values [30, 31, 32, 32, 33, 33, 34, 34, 35] are within [29, 37] ✓\n",
        "- Value 500 >> 37, therefore **500 is a confirmed outlier**\n",
        "\n",
        "**Statistical theory:** Under normal distribution, approximately **99.3%** of data falls within these fences. The multiplier 1.5 balances:\n",
        "- **Sensitivity:** Detecting true outliers\n",
        "- **Specificity:** Avoiding false positives\n",
        "- Stricter rule (3 × IQR) would only flag extreme outliers\n",
        "\n",
        "### **Part 3: Robust vs Non-Robust Statistics**\n",
        "\n",
        "**Comparative Analysis:**\n",
        "\n",
        "| Statistic | Value | Robust? | Interpretation |\n",
        "|-----------|-------|---------|----------------|\n",
        "| **Mean** | 79.4 | ❌ No | Severely distorted by 500 |\n",
        "| **Median** | 33.0 | ✅ Yes | Accurately represents typical spending |\n",
        "| **Std Dev** | 147.8 | ❌ No | Larger than most data values |\n",
        "| **IQR** | 2.0 | ✅ Yes | Shows tight clustering of 50% |\n",
        "\n",
        "**Why Median + IQR is Superior for This Dataset:**\n",
        "\n",
        "1. **Resistance to Outliers:**\n",
        "   - Median uses only **positional information** (middle rank), not magnitude\n",
        "   - One extreme value cannot inflate it\n",
        "   - The outlier 500 doesn't change that 50% of customers spend ≤ $33\n",
        "\n",
        "2. **Representative Central Tendency:**\n",
        "   - Mean = $79.40 misleadingly suggests typical spending is ~$79\n",
        "   - Reality: 9 out of 10 customers (90%) spend $30-35\n",
        "   - Median = $33 accurately captures this typical behavior\n",
        "\n",
        "3. **Meaningful Spread Measure:**\n",
        "   - SD = $147.8 is **larger than most observations**, making it uninformative\n",
        "   - IQR = $2 shows middle 50% of customers concentrated in narrow $32-34 range\n",
        "   - IQR directly interpretable: \"Half the customers spend within $2 of each other\"\n",
        "\n",
        "4. **Handling Skewed Distributions:**\n",
        "   - Data is **heavily right-skewed** due to the outlier\n",
        "   - Mean/SD assume **symmetric distribution** (e.g., normal)\n",
        "   - Median/IQR are **distribution-free** (non-parametric)—work for any shape\n",
        "\n",
        "**Related Statistical Concepts:**\n",
        "\n",
        "- **Breakdown Point:** Median has **50% breakdown point** (can resist up to 50% outliers). Mean has **0% breakdown point** (one extreme value makes it arbitrarily large).\n",
        "\n",
        "- **Parametric vs Non-Parametric:** Mean/SD are parametric (assume distribution shape). Median/IQR are non-parametric (distribution-free).\n",
        "\n",
        "- **Box Plot Visualization:** Perfect for this data—shows Q1, Median, Q3, IQR, and outliers clearly.\n",
        "\n",
        "- **Real-world Application:** In income/wealth data, expenditure analysis, or any domain with occasional extreme values, robust statistics prevent misleading conclusions.\n",
        "\n",
        "**Conclusion:** For this customer spending dataset with an extreme outlier, median and IQR provide accurate, interpretable summaries that reflect typical behavior, while mean and standard deviation are distorted and misleading."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df522116",
      "metadata": {
        "id": "df522116"
      },
      "source": [
        "### Q2. Bayes and Probability in ML (10 marks)\n",
        "\n",
        "A disease affects **1 percent** of people.  \n",
        "A test has:  \n",
        "- Sensitivity: **90 percent**  \n",
        "- Specificity: **92 percent**  \n",
        "\n",
        "A random person tests positive.\n",
        "\n",
        "1. Compute the **positive predictive value (PPV)** using Bayes theorem. Show all steps with probabilities.  \n",
        "2. If prevalence rises to **20 percent**, explain in 3 to 4 sentences whether PPV increases or decreases and why. You may refer to the Bayes formula in words.\n",
        "\n",
        "Write your full answer in the cell below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0cebe10",
      "metadata": {
        "id": "e0cebe10"
      },
      "source": [
        "#### Q2 Answer (Student)\n",
        "\n",
        "---\n",
        "\n",
        "## **DIRECT ANSWER:**\n",
        "\n",
        "**Given:**\n",
        "- Prevalence (disease rate): 1%\n",
        "- Sensitivity: 90%\n",
        "- Specificity: 92%\n",
        "\n",
        "### **1. Positive Predictive Value (PPV) Calculation:**\n",
        "\n",
        "**Step 1:** Define probabilities:\n",
        "- P(Disease) = 0.01\n",
        "- P(No Disease) = 0.99\n",
        "- P(Test+ | Disease) = 0.90 (Sensitivity)\n",
        "- P(Test- | No Disease) = 0.92 (Specificity)\n",
        "- P(Test+ | No Disease) = 1 - 0.92 = 0.08 (False Positive Rate)\n",
        "\n",
        "**Step 2:** Calculate P(Test+) using Law of Total Probability:\n",
        "```\n",
        "P(Test+) = P(Test+ | Disease) × P(Disease) + P(Test+ | No Disease) × P(No Disease)\n",
        "P(Test+) = (0.90 × 0.01) + (0.08 × 0.99)\n",
        "P(Test+) = 0.009 + 0.0792\n",
        "P(Test+) = 0.0882\n",
        "```\n",
        "\n",
        "**Step 3:** Apply Bayes Theorem:\n",
        "```\n",
        "PPV = P(Disease | Test+) = P(Test+ | Disease) × P(Disease) / P(Test+)\n",
        "PPV = (0.90 × 0.01) / 0.0882\n",
        "PPV = 0.009 / 0.0882\n",
        "PPV = 0.102 or 10.2%\n",
        "```\n",
        "\n",
        "**Result:** Only **10.2%** of people who test positive actually have the disease.\n",
        "\n",
        "### **2. Effect of Prevalence Increase to 20%:**\n",
        "\n",
        "When prevalence increases from 1% to 20%, the PPV **increases significantly** to approximately **73.8%**. This happens because Bayes theorem weights the prior probability (prevalence) heavily. With higher prevalence, there are more true positives relative to false positives in the population. The numerator P(Test+ | Disease) × P(Disease) grows faster than the denominator P(Test+), since true positive cases increase substantially while false positives remain proportional to the shrinking healthy population. Thus, a positive test becomes much more reliable evidence of disease when the disease is more common.\n",
        "\n",
        "---\n",
        "\n",
        "## **QUESTION ANALYSIS:**\n",
        "\n",
        "This question tests understanding of:\n",
        "\n",
        "1. **Bayes Theorem application** in medical/ML screening scenarios\n",
        "2. **Conditional probability** manipulation (sensitivity, specificity, PPV)\n",
        "3. **Base rate effect** (how prevalence impacts predictive value)\n",
        "4. **Practical interpretation** of screening test results\n",
        "\n",
        "The counterintuitive result (only 10.2% PPV despite 90% sensitivity) demonstrates the critical importance of base rates in probabilistic reasoning—a fundamental concept in ML classification problems.\n",
        "\n",
        "---\n",
        "\n",
        "## **DETAILED ANSWER ANALYSIS WITH THEORY:**\n",
        "\n",
        "### **Part 1: Computing PPV Using Bayes Theorem**\n",
        "\n",
        "**Understanding the Terms:**\n",
        "\n",
        "1. **Prevalence (Prior Probability):**\n",
        "   - P(Disease) = 0.01 → 1% of population has the disease\n",
        "   - P(No Disease) = 0.99 → 99% are healthy\n",
        "\n",
        "2. **Sensitivity (True Positive Rate):**\n",
        "   - P(Test+ | Disease) = 0.90\n",
        "   - If you have the disease, 90% chance test detects it\n",
        "   - Also called **Recall** in ML classification\n",
        "\n",
        "3. **Specificity (True Negative Rate):**\n",
        "   - P(Test- | No Disease) = 0.92\n",
        "   - If you're healthy, 92% chance test correctly says negative\n",
        "   - Complement: P(Test+ | No Disease) = 0.08 → **False Positive Rate**\n",
        "\n",
        "4. **PPV (Positive Predictive Value):**\n",
        "   - P(Disease | Test+) = ?\n",
        "   - **What we want:** If test says positive, what's the probability you actually have the disease?\n",
        "   - Also called **Precision** in ML classification\n",
        "\n",
        "**Bayes Theorem Formula:**\n",
        "\n",
        "$$P(\\text{Disease} \\mid \\text{Test+}) = \\frac{P(\\text{Test+} \\mid \\text{Disease}) \\times P(\\text{Disease})}{P(\\text{Test+})}$$\n",
        "\n",
        "**Step-by-Step Calculation:**\n",
        "\n",
        "**Step 1:** Calculate total probability of testing positive (denominator):\n",
        "\n",
        "Using **Law of Total Probability**:\n",
        "$$P(\\text{Test+}) = P(\\text{Test+} \\mid \\text{Disease}) \\times P(\\text{Disease}) + P(\\text{Test+} \\mid \\text{No Disease}) \\times P(\\text{No Disease})$$\n",
        "\n",
        "Substituting values:\n",
        "$$P(\\text{Test+}) = (0.90 \\times 0.01) + (0.08 \\times 0.99)$$\n",
        "$$P(\\text{Test+}) = 0.009 + 0.0792 = 0.0882$$\n",
        "\n",
        "**Interpretation:** 8.82% of the total population tests positive (both true and false positives combined).\n",
        "\n",
        "**Step 2:** Apply Bayes Theorem:\n",
        "\n",
        "$$\\text{PPV} = \\frac{0.90 \\times 0.01}{0.0882} = \\frac{0.009}{0.0882} = 0.102$$\n",
        "\n",
        "**Result:** PPV = **10.2%**\n",
        "\n",
        "**What This Means (Confusion Matrix Perspective):**\n",
        "\n",
        "In a population of 10,000 people:\n",
        "- **Disease present:** 10,000 × 0.01 = 100 people\n",
        "  - Test positive (True Positives): 100 × 0.90 = **90**\n",
        "  - Test negative (False Negatives): 100 × 0.10 = 10\n",
        "- **No disease:** 10,000 × 0.99 = 9,900 people\n",
        "  - Test positive (False Positives): 9,900 × 0.08 = **792**\n",
        "  - Test negative (True Negatives): 9,900 × 0.92 = 9,108\n",
        "\n",
        "**Total testing positive:** 90 + 792 = 882 people\n",
        "\n",
        "**PPV = True Positives / All Positives = 90 / 882 = 0.102 = 10.2%**\n",
        "\n",
        "**Key Insight:** Despite high sensitivity (90%), the low prevalence (1%) means false positives (792) far outnumber true positives (90), resulting in low PPV.\n",
        "\n",
        "### **Part 2: Effect of Prevalence Increase to 20%**\n",
        "\n",
        "**Recalculating with 20% Prevalence:**\n",
        "\n",
        "Given:\n",
        "- P(Disease) = 0.20\n",
        "- P(No Disease) = 0.80\n",
        "- Sensitivity and specificity unchanged\n",
        "\n",
        "**Step 1:** New P(Test+):\n",
        "$$P(\\text{Test+}) = (0.90 \\times 0.20) + (0.08 \\times 0.80)$$\n",
        "$$P(\\text{Test+}) = 0.18 + 0.064 = 0.244$$\n",
        "\n",
        "**Step 2:** New PPV:\n",
        "$$\\text{PPV} = \\frac{0.90 \\times 0.20}{0.244} = \\frac{0.18}{0.244} = 0.738$$\n",
        "\n",
        "**New Result:** PPV = **73.8%**\n",
        "\n",
        "**Comparison:**\n",
        "\n",
        "| Prevalence | PPV | Interpretation |\n",
        "|------------|-----|----------------|\n",
        "| 1% | 10.2% | Low reliability—mostly false positives |\n",
        "| 20% | 73.8% | High reliability—mostly true positives |\n",
        "\n",
        "**Why PPV Increases:**\n",
        "\n",
        "1. **More True Positives:**\n",
        "   - At 1% prevalence: 100 people have disease → 90 true positives\n",
        "   - At 20% prevalence: 2,000 people have disease → 1,800 true positives\n",
        "   - True positives increase by **20×**\n",
        "\n",
        "2. **Fewer False Positives:**\n",
        "   - At 1% prevalence: 9,900 healthy → 792 false positives\n",
        "   - At 20% prevalence: 8,000 healthy → 640 false positives\n",
        "   - False positives decrease by ~19%\n",
        "\n",
        "3. **Numerator vs Denominator Growth:**\n",
        "   - In Bayes formula, numerator P(Test+ | Disease) × P(Disease) grows **linearly** with prevalence\n",
        "   - Denominator P(Test+) grows **sublinearly** because:\n",
        "     - True positive contribution increases\n",
        "     - False positive contribution decreases (fewer healthy people)\n",
        "   - **Result:** Ratio increases dramatically\n",
        "\n",
        "**Mathematical Insight:**\n",
        "\n",
        "As prevalence increases:\n",
        "- Numerator: 0.90 × prevalence (linear growth)\n",
        "- Denominator: 0.90 × prevalence + 0.08 × (1 - prevalence)\n",
        "- Denominator grows slower because second term shrinks\n",
        "\n",
        "**ML Classification Analogy:**\n",
        "\n",
        "- **Low prevalence (imbalanced classes):** Even high-recall classifiers have low precision\n",
        "- **High prevalence (balanced classes):** Same classifier achieves both high recall and precision\n",
        "- **Base Rate Effect:** Prior probability strongly influences posterior probability\n",
        "- **Practical implication:** In ML, class imbalance requires careful threshold tuning or resampling\n",
        "\n",
        "**Related Statistical Concepts:**\n",
        "\n",
        "1. **Base Rate Fallacy:** Ignoring prevalence leads to overestimating PPV—common cognitive bias\n",
        "\n",
        "2. **Precision-Recall Tradeoff:** In ML, similar tradeoff between catching all positives (sensitivity/recall) vs. being sure about positives (PPV/precision)\n",
        "\n",
        "3. **ROC vs PR Curves:** ROC curves can be misleading with imbalanced data; Precision-Recall curves better show base rate effects\n",
        "\n",
        "4. **Bayesian Updating:** As we gather more evidence (higher prevalence = stronger prior), posterior probability shifts dramatically\n",
        "\n",
        "**Conclusion:** PPV is critically dependent on disease prevalence (base rate). A test with fixed sensitivity and specificity becomes much more reliable as prevalence increases, because the ratio of true positives to false positives improves. This principle applies directly to ML classification problems with imbalanced datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad2227dd",
      "metadata": {
        "id": "ad2227dd"
      },
      "source": [
        "### Q3. ML Pipeline Thinking (15 marks)\n",
        "\n",
        "Columns in a new dataset:\n",
        "\n",
        "- `age` (numeric)  \n",
        "- `region` (categorical with 7 levels)  \n",
        "- `daily_clicks` (numeric)  \n",
        "- `premium_user` (0 or 1 target)\n",
        "\n",
        "1. Identify the **type of ML task**. (1 or 2 lines)  \n",
        "2. List **four preprocessing steps** that would be reasonable for this dataset. (bullet points are fine)  \n",
        "3. In one short paragraph (6 to 8 sentences), describe the **end to end ML pipeline** you would follow for this problem, from raw data to model evaluation.\n",
        "\n",
        "Write your full answer in the cell below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7a3ead6",
      "metadata": {
        "id": "a7a3ead6"
      },
      "source": [
        "#### Q3 Answer (Student)\n",
        "\n",
        "---\n",
        "\n",
        "## **DIRECT ANSWER:**\n",
        "\n",
        "### **1. Type of ML Task:**\n",
        "**Binary Classification** - Predicting a binary target variable (`premium_user` with values 0 or 1) based on input features.\n",
        "\n",
        "### **2. Four Preprocessing Steps:**\n",
        "- **Handle Missing Values:** Impute missing values in `age` and `daily_clicks` using median or mean imputation\n",
        "- **Encode Categorical Variables:** One-hot encode or label encode the `region` feature (7 levels)\n",
        "- **Feature Scaling:** Standardize or normalize numeric features (`age`, `daily_clicks`) to similar scales\n",
        "- **Handle Class Imbalance:** Check target distribution and apply SMOTE, class weights, or resampling if imbalanced\n",
        "\n",
        "### **3. End-to-End ML Pipeline:**\n",
        "\n",
        "The complete ML pipeline starts with **exploratory data analysis (EDA)** to understand feature distributions, missing values, correlations, and class balance in `premium_user`. Next, **data preprocessing** includes imputing missing values in numeric columns, encoding the categorical `region` variable using one-hot encoding, and scaling features with StandardScaler or RobustScaler. After preprocessing, I would **split the data** into training and test sets (80/20 or 70/30) using stratified sampling to preserve class proportions. For **model selection**, I would train multiple algorithms like Logistic Regression (baseline), Decision Trees, Random Forest, and Gradient Boosting, using cross-validation to tune hyperparameters. **Model evaluation** would involve computing metrics like accuracy, precision, recall, F1-score, and ROC-AUC, with emphasis on metrics appropriate for the business context (e.g., recall if false negatives are costly). Finally, I would **interpret results** using feature importance or coefficients, validate on the test set, and deploy the best model with monitoring for performance degradation. Throughout the pipeline, I'd document decisions and maintain reproducibility through version control and clear code structure.\n",
        "\n",
        "---\n",
        "\n",
        "## **QUESTION ANALYSIS:**\n",
        "\n",
        "This question evaluates understanding of:\n",
        "\n",
        "1. **Problem formulation:** Recognizing binary classification from dataset structure\n",
        "2. **Preprocessing knowledge:** Identifying appropriate data preparation steps\n",
        "3. **ML workflow comprehension:** Articulating the full pipeline from data to deployment\n",
        "4. **Practical ML thinking:** Considering real-world concerns like class imbalance and model interpretability\n",
        "\n",
        "The dataset design (numeric + categorical features, binary target) represents a common real-world ML scenario, testing whether students can translate problem specifications into actionable ML steps.\n",
        "\n",
        "---\n",
        "\n",
        "## **DETAILED ANSWER ANALYSIS WITH THEORY:**\n",
        "\n",
        "### **Part 1: Identifying the ML Task Type**\n",
        "\n",
        "**Binary Classification Task**\n",
        "\n",
        "**Justification:**\n",
        "- **Target variable:** `premium_user` with values {0, 1}\n",
        "- **Two classes:** Non-premium (0) and premium (1) users\n",
        "- **Prediction goal:** Classify new users into one of two categories\n",
        "- **Not regression:** Target is categorical, not continuous\n",
        "\n",
        "**ML Task Taxonomy:**\n",
        "- **Supervised Learning:** We have labeled data (target `premium_user` is known)\n",
        "- **Classification:** Predicting discrete categories\n",
        "- **Binary:** Exactly two classes (vs. multi-class with 3+ categories)\n",
        "\n",
        "**Alternative Perspective:**\n",
        "Could be framed as **probability estimation** (predicting P(premium_user=1)), but the fundamental task is still binary classification.\n",
        "\n",
        "**Related ML Concepts:**\n",
        "- **Logistic Regression:** Natural baseline for binary classification\n",
        "- **Decision boundary:** Algorithms learn to separate two classes in feature space\n",
        "- **Threshold tuning:** Can adjust probability threshold (default 0.5) based on business needs\n",
        "\n",
        "### **Part 2: Four Essential Preprocessing Steps**\n",
        "\n",
        "**1. Handle Missing Values**\n",
        "\n",
        "**Why important:**\n",
        "Most ML algorithms cannot handle NaN/null values directly and will either error or silently drop incomplete rows, losing valuable data.\n",
        "\n",
        "**Strategies:**\n",
        "- **Numeric features** (`age`, `daily_clicks`):\n",
        "  - **Median imputation:** Robust to outliers (preferred for skewed data)\n",
        "  - **Mean imputation:** Simple, works for symmetric distributions\n",
        "  - **Predictive imputation:** Use other features to predict missing values (advanced)\n",
        "- **Categorical features** (`region`):\n",
        "  - **Mode imputation:** Fill with most frequent category\n",
        "  - **Create \"Missing\" category:** Preserve missingness as information\n",
        "\n",
        "**When to apply:** After EDA but before encoding/scaling\n",
        "\n",
        "**2. Encode Categorical Variables**\n",
        "\n",
        "**Why important:**\n",
        "ML algorithms require numeric inputs; categorical variables must be converted to numerical representations.\n",
        "\n",
        "**For `region` (7 levels, nominal categorical):**\n",
        "\n",
        "**One-Hot Encoding (Preferred):**\n",
        "- Creates 7 binary columns (or 6 with drop_first=True to avoid multicollinearity)\n",
        "- Each column represents one region: [1, 0, 0, 0, 0, 0, 0]\n",
        "- **Pros:** No ordinal assumption, works with all algorithms\n",
        "- **Cons:** Increases dimensionality (curse of dimensionality with many categories)\n",
        "\n",
        "**Label Encoding (Alternative):**\n",
        "- Maps regions to integers: {0, 1, 2, 3, 4, 5, 6}\n",
        "- **Pros:** Lower dimensionality, works for tree-based models\n",
        "- **Cons:** Introduces false ordinality (model thinks region 6 > region 0)\n",
        "\n",
        "**Target Encoding (Advanced):**\n",
        "- Encode based on target mean per category\n",
        "- Reduces dimensionality but risks data leakage\n",
        "\n",
        "**When to apply:** After missing value imputation, before scaling\n",
        "\n",
        "**3. Feature Scaling**\n",
        "\n",
        "**Why important:**\n",
        "Algorithms using distance metrics (Logistic Regression, SVM, KNN, Neural Networks) are sensitive to feature magnitudes. Unscaled features can dominate model training.\n",
        "\n",
        "**Example problem:**\n",
        "- `age` range: [18, 80] (scale: ~60)\n",
        "- `daily_clicks` range: [0, 10000] (scale: ~10000)\n",
        "- Without scaling, `daily_clicks` dominates distance calculations\n",
        "\n",
        "**Scaling Methods:**\n",
        "\n",
        "**StandardScaler (Z-score normalization):**\n",
        "- Formula: $z = \\frac{x - \\mu}{\\sigma}$\n",
        "- Results in mean=0, std=1\n",
        "- **When:** Data is approximately normal, few outliers\n",
        "\n",
        "**RobustScaler:**\n",
        "- Uses median and IQR instead of mean and std\n",
        "- Formula: $z = \\frac{x - \\text{median}}{\\text{IQR}}$\n",
        "- **When:** Data has outliers (robust to extremes)\n",
        "\n",
        "**MinMaxScaler:**\n",
        "- Scales to [0, 1] range\n",
        "- **When:** Need bounded output (e.g., for neural networks)\n",
        "\n",
        "**Important:** \n",
        "- Fit scaler on training data only, then transform both train and test\n",
        "- One-hot encoded features don't need scaling (already 0/1)\n",
        "- Tree-based models (Random Forest, XGBoost) don't require scaling\n",
        "\n",
        "**When to apply:** After encoding, before model training\n",
        "\n",
        "**4. Handle Class Imbalance**\n",
        "\n",
        "**Why important:**\n",
        "If `premium_user=1` represents only 5% of data, a naive model can achieve 95% accuracy by always predicting 0, but this is useless for business.\n",
        "\n",
        "**Detection:**\n",
        "Check target distribution: `df['premium_user'].value_counts(normalize=True)`\n",
        "\n",
        "**Mitigation Strategies:**\n",
        "\n",
        "**Resampling:**\n",
        "- **Oversampling minority:** Duplicate premium_user=1 samples\n",
        "- **Undersampling majority:** Remove premium_user=0 samples\n",
        "- **SMOTE:** Synthetic Minority Over-sampling Technique (creates synthetic examples)\n",
        "\n",
        "**Algorithm-level:**\n",
        "- **Class weights:** Penalize misclassifying minority class more (e.g., `class_weight='balanced'` in sklearn)\n",
        "- Adjusts loss function: $L = w_0 \\cdot L_0 + w_1 \\cdot L_1$\n",
        "\n",
        "**Metric-aware:**\n",
        "- Don't rely on accuracy alone\n",
        "- Use F1-score, precision-recall curves, ROC-AUC\n",
        "- **Recall** important if missing premium users is costly\n",
        "\n",
        "**Threshold tuning:**\n",
        "- Lower threshold for positive class (e.g., 0.3 instead of 0.5)\n",
        "- Increases recall at expense of precision\n",
        "\n",
        "**When to apply:** After train-test split, before model training\n",
        "\n",
        "### **Part 3: End-to-End ML Pipeline**\n",
        "\n",
        "**Complete Pipeline Architecture:**\n",
        "\n",
        "**Phase 1: Data Understanding (EDA)**\n",
        "\n",
        "**Goals:**\n",
        "- Understand feature distributions\n",
        "- Identify data quality issues\n",
        "- Detect patterns and relationships\n",
        "\n",
        "**Tasks:**\n",
        "- **Summary statistics:** `df.describe()`, `df.info()`\n",
        "- **Missing values:** `df.isnull().sum()`\n",
        "- **Target distribution:** Check class balance\n",
        "- **Feature distributions:** Histograms for `age`, `daily_clicks`\n",
        "- **Categorical analysis:** Bar plot for `region` frequencies\n",
        "- **Correlations:** Heatmap of numeric features vs. target\n",
        "- **Outlier detection:** Box plots, IQR analysis\n",
        "\n",
        "**Tools:** pandas, matplotlib, seaborn\n",
        "\n",
        "**Phase 2: Data Preprocessing**\n",
        "\n",
        "**Pipeline steps in order:**\n",
        "1. Handle missing values (imputation)\n",
        "2. Encode categorical variables (one-hot encoding)\n",
        "3. Split data (train/test stratified split)\n",
        "4. Scale numeric features (fit on train only)\n",
        "5. Address class imbalance (SMOTE or class weights)\n",
        "\n",
        "**Scikit-learn Pipeline:**\n",
        "```python\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', LogisticRegression())\n",
        "])\n",
        "```\n",
        "\n",
        "**Phase 3: Train-Test Split**\n",
        "\n",
        "**Strategy:**\n",
        "- **Split ratio:** 80/20 or 70/30\n",
        "- **Stratification:** Use `stratify=y` to preserve class proportions\n",
        "- **Random state:** Set seed for reproducibility\n",
        "\n",
        "**Code:**\n",
        "```python\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "```\n",
        "\n",
        "**Phase 4: Model Selection and Training**\n",
        "\n",
        "**Baseline Model:**\n",
        "- Start with **Logistic Regression** (simple, interpretable, fast)\n",
        "- Establishes performance benchmark\n",
        "\n",
        "**Additional Models:**\n",
        "- **Decision Trees:** Handle non-linearity, interpretable\n",
        "- **Random Forest:** Ensemble method, reduces overfitting\n",
        "- **Gradient Boosting (XGBoost/LightGBM):** Often best performance\n",
        "- **SVM:** Good for complex decision boundaries\n",
        "- **Neural Networks:** For large datasets with complex patterns\n",
        "\n",
        "**Cross-Validation:**\n",
        "- Use 5-fold or 10-fold CV on training set\n",
        "- Estimates generalization performance\n",
        "- Prevents overfitting to validation set\n",
        "\n",
        "**Hyperparameter Tuning:**\n",
        "- **Grid Search:** Exhaustive search over parameter grid\n",
        "- **Random Search:** Sample parameter combinations randomly\n",
        "- **Bayesian Optimization:** Smart search (advanced)\n",
        "\n",
        "**Phase 5: Model Evaluation**\n",
        "\n",
        "**Metrics for Binary Classification:**\n",
        "\n",
        "**Confusion Matrix:**\n",
        "```\n",
        "                Predicted\n",
        "                0       1\n",
        "Actual  0      TN      FP\n",
        "        1      FN      TP\n",
        "```\n",
        "\n",
        "**Key Metrics:**\n",
        "- **Accuracy:** (TP + TN) / Total - Overall correctness\n",
        "- **Precision:** TP / (TP + FP) - Of predicted premium, how many are correct?\n",
        "- **Recall (Sensitivity):** TP / (TP + FN) - Of actual premium, how many caught?\n",
        "- **F1-Score:** Harmonic mean of precision and recall\n",
        "- **ROC-AUC:** Area under ROC curve - Overall discrimination ability\n",
        "\n",
        "**Metric Selection Based on Business Context:**\n",
        "- **High recall priority:** Can't afford to miss premium users (e.g., churn prevention)\n",
        "- **High precision priority:** Marketing budget limited, target only confident predictions\n",
        "- **Balanced F1:** General-purpose metric\n",
        "\n",
        "**ROC and Threshold Analysis:**\n",
        "- Plot ROC curve (TPR vs. FPR)\n",
        "- Identify optimal threshold for business needs\n",
        "- AUC = 0.5 (random), AUC = 1.0 (perfect)\n",
        "\n",
        "**Phase 6: Model Interpretation**\n",
        "\n",
        "**Feature Importance:**\n",
        "- **Tree models:** Built-in feature_importances_\n",
        "- **Linear models:** Coefficient magnitudes\n",
        "- **SHAP values:** Model-agnostic interpretation (advanced)\n",
        "\n",
        "**Business Insights:**\n",
        "- Which features most predict premium_user?\n",
        "- Is `age` or `daily_clicks` more important?\n",
        "- Regional patterns in premium adoption?\n",
        "\n",
        "**Phase 7: Test Set Validation**\n",
        "\n",
        "**Final evaluation:**\n",
        "- Run best model on held-out test set\n",
        "- Report all metrics\n",
        "- Compare to cross-validation results\n",
        "- Large discrepancy indicates overfitting\n",
        "\n",
        "**Phase 8: Deployment and Monitoring**\n",
        "\n",
        "**Deployment:**\n",
        "- Serialize model (pickle, joblib)\n",
        "- Create prediction API (Flask, FastAPI)\n",
        "- Integrate with production systems\n",
        "\n",
        "**Monitoring:**\n",
        "- Track prediction distribution (data drift)\n",
        "- Monitor performance metrics over time (model drift)\n",
        "- Set up alerts for degradation\n",
        "- Plan for model retraining\n",
        "\n",
        "**Pipeline Best Practices:**\n",
        "\n",
        "1. **Reproducibility:**\n",
        "   - Version control (Git)\n",
        "   - Set random seeds\n",
        "   - Document dependencies (requirements.txt)\n",
        "\n",
        "2. **Modularity:**\n",
        "   - Separate EDA, preprocessing, training, evaluation\n",
        "   - Use sklearn Pipeline for preprocessing steps\n",
        "\n",
        "3. **Validation:**\n",
        "   - Never touch test set until final evaluation\n",
        "   - Use cross-validation for model selection\n",
        "\n",
        "4. **Documentation:**\n",
        "   - Document assumptions and decisions\n",
        "   - Explain metric choices\n",
        "   - Record hyperparameter tuning results\n",
        "\n",
        "5. **Error Analysis:**\n",
        "   - Examine misclassified examples\n",
        "   - Identify systematic failures\n",
        "   - Iterate on features or model\n",
        "\n",
        "**Related ML Concepts:**\n",
        "\n",
        "- **Feature Engineering:** Creating derived features (e.g., `age_group`, `click_rate`)\n",
        "- **Ensemble Methods:** Combining multiple models (stacking, blending)\n",
        "- **AutoML:** Automated pipeline search (TPOT, Auto-sklearn)\n",
        "- **MLOps:** Production ML infrastructure and monitoring\n",
        "- **A/B Testing:** Validating model impact in production\n",
        "\n",
        "**Conclusion:** A robust ML pipeline integrates data understanding, systematic preprocessing, rigorous model selection, appropriate evaluation, and production considerations. For this binary classification problem with mixed feature types, following this end-to-end pipeline ensures reliable, interpretable, and deployable results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26ac9d44",
      "metadata": {
        "id": "26ac9d44"
      },
      "source": [
        "---\n",
        "\n",
        "## SECTION B – Applied Coding Problems (60 Marks)\n",
        "\n",
        "Answer **all three questions** in this section.\n",
        "\n",
        "- Write clean, commented code.  \n",
        "- After each main step, add a short markdown explanation of what you did and what you observe.  \n",
        "- Make sure all plots are visible in the notebook.\n",
        "\n",
        "Marks for each question are clearly mentioned.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "831e60e5",
      "metadata": {
        "id": "831e60e5"
      },
      "source": [
        "### Q4. Applied EDA and Preprocessing (20 marks)\n",
        "\n",
        "We work with the following dataset:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"age\": [25, 30, None, 22, 45, 52, None],\n",
        "    \"region\": [\"north\",\"south\",\"north\",\"east\",\"west\",\"west\",\"south\"],\n",
        "    \"purchases\": [3, 10, 5, None, 20, 18, 9],\n",
        "    \"premium_user\": [0,1,0,0,1,1,0]\n",
        "})\n",
        "```\n",
        "\n",
        "**Tasks (20 marks total):**\n",
        "\n",
        "1. **Compact EDA using pandas** (6 marks)  \n",
        "   - Show missing value summary for each column.  \n",
        "   - Show number of unique values per column.  \n",
        "   - Show correlation among numeric columns.  \n",
        "   - Plot:  \n",
        "     - A histogram of `purchases`.  \n",
        "     - A bar chart for `region` frequency.\n",
        "\n",
        "2. **Preprocess the dataset using sklearn plus pandas** (9 marks)  \n",
        "   - Impute `age` with **median**.  \n",
        "   - Impute `purchases` with **mean**.  \n",
        "   - One hot encode `region`.  \n",
        "   - Scale all numeric columns using **RobustScaler**.\n",
        "\n",
        "3. **Create one domain driven feature** (3 marks)  \n",
        "   - Example ideas: `high_spender` based on `purchases`, or `age_group` bins, or an interaction like `age * purchases`.\n",
        "\n",
        "4. Show the **final transformed dataframe** ready for model training. (2 marks)\n",
        "\n",
        "Use short explanations in markdown to describe each main block of code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4145df0b",
      "metadata": {
        "id": "4145df0b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>region</th>\n",
              "      <th>purchases</th>\n",
              "      <th>premium_user</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>25.0</td>\n",
              "      <td>north</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>30.0</td>\n",
              "      <td>south</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>north</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>22.0</td>\n",
              "      <td>east</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>45.0</td>\n",
              "      <td>west</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>52.0</td>\n",
              "      <td>west</td>\n",
              "      <td>18.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>NaN</td>\n",
              "      <td>south</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    age region  purchases  premium_user\n",
              "0  25.0  north        3.0             0\n",
              "1  30.0  south       10.0             1\n",
              "2   NaN  north        5.0             0\n",
              "3  22.0   east        NaN             0\n",
              "4  45.0   west       20.0             1\n",
              "5  52.0   west       18.0             1\n",
              "6   NaN  south        9.0             0"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Q4 – Student Answer\n",
        "\n",
        "# Step 1: Create the dataframe\n",
        "df = pd.DataFrame({\n",
        "    \"age\": [25, 30, None, 22, 45, 52, None],\n",
        "    \"region\": [\"north\",\"south\",\"north\",\"east\",\"west\",\"west\",\"south\"],\n",
        "    \"purchases\": [3, 10, 5, None, 20, 18, 9],\n",
        "    \"premium_user\": [0,1,0,0,1,1,0]\n",
        "})\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c009f9e",
      "metadata": {
        "id": "3c009f9e"
      },
      "source": [
        "## **PART 1: Compact EDA using Pandas (6 marks)**\n",
        "\n",
        "We'll perform exploratory data analysis to understand the dataset's structure, missing values, distributions, and relationships."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2a065bbd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "MISSING VALUE SUMMARY\n",
            "==================================================\n",
            "              Missing_Count  Missing_Percentage\n",
            "age                       2               28.57\n",
            "region                    0                0.00\n",
            "purchases                 1               14.29\n",
            "premium_user              0                0.00\n",
            "\n",
            "Total rows: 7\n",
            "Total missing values: 3\n"
          ]
        }
      ],
      "source": [
        "# EDA Step 1: Missing Value Summary\n",
        "print(\"=\" * 50)\n",
        "print(\"MISSING VALUE SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "missing_summary = pd.DataFrame({\n",
        "    'Missing_Count': df.isnull().sum(),\n",
        "    'Missing_Percentage': (df.isnull().sum() / len(df) * 100).round(2)\n",
        "})\n",
        "print(missing_summary)\n",
        "print(f\"\\nTotal rows: {len(df)}\")\n",
        "print(f\"Total missing values: {df.isnull().sum().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3db8bf31",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "UNIQUE VALUES PER COLUMN\n",
            "==================================================\n",
            "              Unique_Count    Dtype\n",
            "age                      5  float64\n",
            "region                   4   object\n",
            "purchases                6  float64\n",
            "premium_user             2    int64\n",
            "\n",
            "Region categories: ['north' 'south' 'east' 'west']\n"
          ]
        }
      ],
      "source": [
        "# EDA Step 2: Unique Values per Column\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"UNIQUE VALUES PER COLUMN\")\n",
        "print(\"=\" * 50)\n",
        "unique_summary = pd.DataFrame({\n",
        "    'Unique_Count': df.nunique(),\n",
        "    'Dtype': df.dtypes\n",
        "})\n",
        "print(unique_summary)\n",
        "print(\"\\nRegion categories:\", df['region'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0cb50427",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "CORRELATION MATRIX (Numeric Columns)\n",
            "==================================================\n",
            "                age  purchases  premium_user\n",
            "age           1.000      0.924         0.790\n",
            "purchases     0.924      1.000         0.826\n",
            "premium_user  0.790      0.826         1.000\n",
            "\n",
            "Correlation between age and purchases: 0.924\n"
          ]
        }
      ],
      "source": [
        "# EDA Step 3: Correlation Among Numeric Columns\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"CORRELATION MATRIX (Numeric Columns)\")\n",
        "print(\"=\" * 50)\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "correlation_matrix = df[numeric_cols].corr()\n",
        "print(correlation_matrix.round(3))\n",
        "print(f\"\\nCorrelation between age and purchases: {df[['age', 'purchases']].corr().iloc[0, 1]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2e8a883c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAGGCAYAAACqvTJ0AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAa2hJREFUeJzt3QmYU9X5x/F3GLZhlx0EZBRFBQHFQnFfEES0oFbRakFEXFEUV6yACErdEFQUtQJS69oq/lsVFxSVilLAXUHBkUV2UNZhm8n/+Z2SNDOTmclkJjfJzffzPGFIcnOTe85d3vvec87NCAQCAQMAAAAAAAA8VMnLLwMAAAAAAACEpBQAAAAAAAA8R1IKAAAAAAAAniMpBQAAAAAAAM+RlAIAAAAAAIDnSEoBAAAAAADAcySlAAAAAAAA4DmSUgAAAAAAAPAcSSkAAAAAAAB4jqQUkKLuvPNOy8jI8OS7TjrpJPcImj17tvvuv//97558/yWXXGKtW7e2ZLZt2za77LLLrGnTpq5srr/+eksV+r1DhgxJ9M8AAMCzWAaQadOmuTjop59+8jye1Xfqux944AHfnTsAZUFSCkiiA2LwUb16dWvevLn17NnTHn74Ydu6dWuFfM+qVavcAenzzz+3ZJPMvy0a99xzj6vHq666yv7617/aH//4x2KnVUASXt+NGze2448/3l599VVPfzMAAImKdSpXrmz777+/O1H/+eeffVsRwQt5kR4XXHBBon+er8u6WrVq1qRJE5eMVJy2fv36CvmeHTt2uJhV35dskvm3AcWpXOw7ADx31113WXZ2tu3Zs8fWrFnjDihqcTN+/Hj7v//7P+vQoUNo2jvuuMNuu+22Mid+Ro8e7ZIinTp1ivpzb7/9tsVbSb/tqaeesvz8fEtm7733nv32t7+1UaNGRTW9lvHGG28MLfsTTzxh55xzjj3++ON25ZVXxvnXAgCQ2Fhn586d9sknn7hk1Zw5c+zrr792F+XixYtYpiTXXXed/eY3vynwWrK3Ak9VwbLOy8tziaiPP/7YxWeKp1966SU75ZRTQtPqIqKSg0pglSXxo5hVytL6zot4tqTfFsu5A+AFklJAEunVq5cdffTRoefDhw93yY4zzzzTfve739l3331nWVlZ7j1dYdQj3ge2GjVqWNWqVS2RqlSpYslu3bp1dvjhh0c9va4OX3zxxaHn/fv3tzZt2thDDz1UIUmp7du3W82aNcs9HwAA4hXrqNt7w4YN7d5773UX384///y4FXaiYxm1iP79738f1bR79+51yYtE/+ZUFamsv/jiC+vRo4ede+659u2331qzZs3c65mZme4RT8GYLNHxrBfnDkAs6L4HJDldzRkxYoQtW7bMnn322RL7hb/zzjt23HHHWb169axWrVrWtm1bu/322917anUVvEI3cODAUNNmXaEMXk1p3769LViwwE444QSXjAp+trhxGHQFStNoHCUdbJU4W7FiRZGrgGqaX1j4PEv7bZHGlNIBXi2NWrZs6a5uaVnVJz8QCEQcL2nGjBlu+TRtu3btbObMmVEnmwYNGuSaf+sKbseOHe2ZZ54p0lQ8JyfHXn/99dBvL+vYBCrDww47zM0nfL6Fm18Hxx8Ilk2wfFTfS5cutTPOOMNq165tF110kXtPQe3EiRPtiCOOcL+/UaNGdvrpp9v8+fOL/IbSykjr4NVXX+3KWsnRBg0a2HnnnVdkWdXST1fpDj74YPedmk7rpdbPcIsWLXJBY/369d10OknRSUks8wIApG4CQXQMK+sxQr788ks78cQT3XGpRYsWNnbsWJs6dWqRY3GkWKa0Y3zhcX+efPJJO+igg9xxUnHLf/7zn3Ivf/j8J0yYEJq/EidlKYdvvvnGxYzh5TBlypQi5aDniiELixSv/frrr67FfjDW0sUzJRDDW/uUtXy0PEo+Kh7Rb1VM8ac//cm99/7777t5RRrO4LnnnnPvzZ0712KhulX5apkeffTREseUUoykITSUMNVvVMu+Sy+9NLS8+u2i+CQY9wXLtKSYrKQxUnVR8oADDnDfp/VZLQfDFReLh8+ztN8W6dxBCdAxY8aE6k3zUmy/a9euAtPpdV0kV6vGLl26uHXxwAMPtOnTp5ehFoDISJUCKUBNi3WAUNPzwYMHR5xGwYgOFurip6bxOrAsWbLE/v3vf7v3lfDQ6yNHjrTLL788FAQec8wxoXls3LjRXcFUM2a14lGQVpK7777bHdxuvfVWF9jpYN+9e3c3LlSwRVc0ovlt4ZR4UgJMwYuCSXWFe+utt+zmm29241LowB5OB9BXXnnFJVQUHGicLl0pW758uUtyFCc3N9cFACpHJbYUlLz88ssuAFBQM3ToUPfbNYbUDTfc4ILAYJe8YFAQLSVflNAr6feUREGFAiglbBQYKqkoKh8FXKpXXZHWdB999JHrMhHeKi+aMlJwqSbwWj+0rAp+1N1QZaTgOfidCnrGjRvnvk+By5YtW1yAt3DhQjvttNNC6+uxxx7rWoypKbmSmmpS37dvX/vHP/5hZ599dtTzAgCkrmAyYL/99gu9Fu0xQsf8k08+2cUial2u6f7yl79E1RUrmmN84aSIxvi84oor3Pfdd999rtv9jz/+GFULGH12w4YNBV5ToilIiTR1aVQcpN+v96ItBw35oHLQMT44nRJEZYnFIrWWV3JEZaxlbtWqlYsBVM6rV692MV9Zy0cJRMV4eq7lVKJDyZt//vOfLqZUfSgB9re//S20bEF6TYmTbt26xbxMSu4pLlI8re+LRPGsWlQpjlNZ6kKv1lHFSKLXFftoDFH9Ri2jhA+xUVxMVhwldlR211xzjVsHdDFRCcavvvqq1Fg8XDS/rTDFV0rEqmwUw3766acu7lLvjMLJQW0rwTIcMGCAS3pqe+ncubO7mAnELAAg4aZOnarmPYH//Oc/xU5Tt27dwJFHHhl6PmrUKPeZoIceesg9X79+fbHz0Pw1jb6vsBNPPNG9N3ny5Ijv6RH0/vvvu2n333//wJYtW0Kvv/TSS+71iRMnhl474IADAgMGDCh1niX9Nn1e8wmaMWOGm3bs2LEFpvv9738fyMjICCxZsiT0mqarWrVqgde++OIL9/ojjzwSKMmECRPcdM8++2zotd27dwe6desWqFWrVoFl1+/r3bt3ifMLn7ZHjx6urvTQ77ngggvcd1177bUFylh/w+Xk5BQpJ5WPXrvtttsKTPvee++516+77roivyE/P7/MZbRjx44i85k7d66bbvr06aHXOnbsWGpZnHrqqYEjjjgisHPnzgK/6ZhjjgkcfPDBZZoXACB1Yp13333XHftWrFgR+Pvf/x5o1KhRoFq1au55WY8ROmbquP/ZZ5+FXtu4cWOgfv367rt0zCwu7oj2GB887jZo0CCwadOm0LSvvfaae/2f//xnicsdPJ5HemjewfnXqVMnsG7dugKfjbYcrr/+ejePTz/9NPSa5qXYsXA56LliyMIKx2tjxowJ1KxZM/D9998XmE6xRmZmZmD58uVlLp8TTjghULt27cCyZcuKjUmGDx/u1odff/21wLJUrlw54u+OVNYvv/xysdMorthvv/2KrJfBMnr11VdLjcm1/hZXjsXFZJHi2WDZZWVlBVauXBl6XfWo12+44YZi19/i5lnSbyt87vD555+755dddlmB6W666Sb3uuLIIH2HXvvwww8L1Ivq6sYbbyympIDo0H0PSBFqClzSXfh0JUdee+21mAdR1JU5dZ+LlsZBUquaIF09UR/9N954w+JJ81f/fw1kGU5XeBRvvfnmmwVeV+stXV0L0hWjOnXquKt3pX2PutVdeOGFodd0dU/fu23bNvvggw9iXgZdpdMVLT3UpFxXZ9UiTs3iY6UrY+F0FVVXKyMNvl64+XY0ZRR+xVUtu9SyTk35te6p5VKQnuvq7g8//BDxd27atMmNlabm+8Erx3pofrqyqM8F78RU2rwAAKlFxxsd+9QiRnGDWvWoO5pa4Jb1GKFu5mo5E36DFLUwCnaXqshjfL9+/Qq05gq26i4tlghSa3B1PQ9/6PuD1Do5vJV1WcpBy6KbrahFcZDmFU05FEdxiZZRyxz8bj1Ufxq+4cMPPyxT+WjAcX1G3eDU6qq4mESxpbqO/f3vfw+99uKLL7rWR+FjccY7nv7Xv/7lYp2KislKopZvag0XpHrs2rWrJ/G0DBs2rMDrwVb/GpYinMZODdZrcB1T98totwGgOCSlgBShACk8AVSYggE18VYzXDX1VRcrNfEuS4JKB8SyDKqpcX4KBxVKUpR1PKWy0thGzZs3L1Ie6koXfD9c4eBHFDj98ssvpX6PlrFSpUpRfU9ZKNhQQPruu++65vAK9NR8O9am9hq4MhjQB6lJvMopvHtAcaIpI3V1UFAdHFtCYy0oIFE3h82bN4emU1dMvXbIIYe4sazUrVJN9sObfyt5qLHSgom54COYQFPz+WjmBQBILZMmTXLHPyUdNOaOjn/h3e3KcozQcVhxR2GRXivvMb7wcTKYgCktlgjSMUwJnfBH+N0G1X0wXFnLoXBMJkoYxEpJLyX9Cn+3fnf4d0dbPsHEhcauLMmhhx7qxqNSd70g/V9Jt2jqtbzxtLosKkGoMZkU5/Tp08d1rSw8xlJZY7KSRKo7xT1exNNa/wuXq5KlSs5VVDwNlIYxpYAUsHLlSnfSX9LBWMkMXYHSOEu6sqFAQleW1CddrXKiubNIecYeKE7hFjlBusoW77udBBX3PYUHRfeSAp1gYFfWcotEAX3hwLqiy+jaa691gZkGPdWV6bp167rfqQRoePJTA+UrIaZWe1r3NL6HxvmaPHmyS5oGp73pppvc1d5Igut6afMCAKQWtQIJjmmoFiIad+cPf/iDLV682LViKcsxwkvxjiUKx2Bel0Ph+ELfr7Ebb7nllojTK2kSr/JRaymN6aX4V8kgjYMZPjh5rNTy6fvvvy8xMaa4RglTfafGutKYpWrd9eCDD7rXtI6WprwxWXG/K1JZFhcXlnXeqRpPwx9ISgEpQANpS3FBSZAOgKeeeqp7jB8/3u655x53RxMlqpQAifagE63CXap0UNKVvfABFXUFRS1dCtPVF921I6gsv013J1ELIzW/Dr/apTu6BN+vCJqPWuUoMAsPLir6eyIJXmEsXHZlaZ2l7ngKptQFIJrWUqVRkKaBLRWYBWlAzkj1q+9TV1A9dFVSySUNWq5EUrDe1U2ipMRcNPMCAKQuneRqUGUN0q2kgwaWLssxQsdhxR2FRXotmY7x0ShrOUTq5q5EX2GR4rLdu3e7wcsLxxA65kZznC7L8hS+q1wkutilLmXPP/+8a6WtMlCPgIqIYzS/0uJpUcssPTQgugZxV1fIF154wcUe8Y6nRcmz8Dv1qd4idZMrHBeWNZ7W+q/vD7YQlLVr17p1JNHbANIH3feAJKfxBHSrVjXrLmlsACUeCguOsRBscqxxGyRSEiEWwbuFhB/sFdToTm/hQY2uLCngCVI/fd1pLlxZfpua++vKUOGrZmpBo4Nx+PeXh75Hd7RRi7MgjWnwyCOPuCtlauIdLwoEFKwXHrPhsccei3oean6uRKGaoFfEVS39nsKfU1kUvkqn8S7Cqax0NTe4HjZu3NjdYeeJJ54oEgQHx52Idl4AgNSm44FaT+lubrrQUZZjhJILc+fOdXf9DY+Hwrt+JeMxPhplKQcti2KtefPmFXg/UjkoLiscW+hOfYWP5RrLSmWri1uFKVZTWZWFuv7popLu2KY7+4YrHFuoNbliuWeffdYtw+mnn+5eK48vvvjCtfRWckd3uSuOuqIV/j2F4+ng3fQqKp6eMWNGaHwwUT3qLniF42klTMPrXcsUvMt2UFl+m9YbKXwnRV3Ylt69e8e8TEBZ0FIKSCIaoFsHHB3odZVCCSmNu6AEhQYBDR97oDCNvaMgQwcQTa++/kpgqE+7msYHD2jqI66uT2phpESQxjYqPI5BtNSCRfNWCxb9Xh3UlDAYPHhwaBpdUVKySgGFAhx1xVKQET6odll/21lnneWuqqoVmPrba6Bwde1SFy8FHIXnHSvdrljBoG53u2DBAnfFSsuiAEDLWtKYBOWlrnHnnXeeC46VaNMyKZlXeAyHkqiMNHj6ww8/7K6CqQ50Reyjjz5y7+kW2GVx5plnulZ7+m0a7FLBqlqsNWjQoMB0ek+BtG4RrHVk/vz5rtzCv09jimjd0RgbWl90BVXrkOap5voKtKKdFwAgtWm8QB3zpk2bZldeeWXUxwh1LVNMoW5m6mKu2EHdvDX2jZJTJbUaSeQxPlplKQcdn3WcV7c3lYMSTcHWYOEUl6mMdeFK5aZ5KPFUOOmjOlHsqWO/ykjH4e3bt9tXX33lyknxV1kTRYpHtDxHHXWUK3/FeJqPhp0ITywGu/BpIHzRxdmyUJyjBKcSbbq4pTrVsih+efXVVwsMMF/YM8884+Lns88+28Veuvj61FNPuZu/BJM46mqp+EQJTXVjVHyiLoGljZdVHMXOKhcNjq7El9Y/xVbhXSfVhVDJIiViBw0a5OJBxczt2rWzLVu2hKYry29T/KwW8FpXlMRSIlYJMZWButYqVgQ8EeVd+gDEUfB2tMFH1apVA02bNg2cdtppgYkTJ4ZuS1zSbV1nzZoV6NOnT6B58+bu8/p74YUXFrmVr27Re/jhh7tb6+rz+m7RbWbbtWsX8fcVvg1t8Ja7zz//vLt1b+PGjd3tbHv37l3kNr/y4IMPBvbff39329hjjz02MH/+/Ii3ti3utxW+3a1s3brV3SpXy1mlShV3a+T777+/wG2FRfO55pprSr31cXHWrl0bGDhwYKBhw4auXHVr5uDvKjw/LX80op1Wt/U999xzAzVq1HC3L77iiisCX3/9dYGyES2Hbtscyd69e125HHrooe7369bbvXr1CixYsKDMZfTLL7+EykK3y+7Zs2dg0aJFRaYbO3ZsoEuXLoF69eq59ULffffdd7tbbYdbunRpoH///m5dVx1qHTnzzDPdLcLLOi8AQGrEOv/5z3+KvJeXlxc46KCD3EPHrWiPEfLZZ58Fjj/+eBdjtGjRIjBu3LjAww8/7L5rzZo1oekixR3RHONzcnLcvHQsLUyvKx4rSTBmevnllyO+X9L8y1IOX375pVu+6tWru2nGjBkTePrpp9289R3hZX3rrbe6ZVZ8oWP5kiVLIsZFirUU57Vp08aVjz5zzDHHBB544IHQcbis5aM45uyzz3bHdf3Wtm3bBkaMGFHks7t27XKxT926dQO5ubmBaATLOvhQeSnuOeGEE1zssG7dumLXy2AZLVy40MXPrVq1cuuUYlyVt2LXcB9//HGgc+fOrlzCl7OkmKxwPBtedoqVW7Zs6b5T6/MXX3xR5PPPPvts4MADD3Tf2alTp8Bbb70VMUYu7rcVPneQPXv2BEaPHh3Izs525aXfoDrfuXNnVLFrpO0KKKsM/eNN+gsAAAAA4kutptUKSmMieXVTlWSklmdqzZ6Tk1NgfKJUoF4DuoOwWsc//fTTif45AOKIMaUAAAAApCQNXB1O3bXUlU3dodI5IZXqNM6Sxk9SNz4A/saYUgAAAABSUrdu3dzYg7p7mMZbUqsajbEzYsSIRP80xEADfGscLI0jdeSRRyZ8wHkA8UdSCgAAAEBK0uDTGnhbgzVrYHMNoq3ElO70htTz+OOPu8Hrdcc7dT8E4H+MKQUAAAAAAADPMaYUAAAAAAAAPEdSCgAAAAAAAJ5jTKkI8vPzbdWqVVa7dm3XNx0AAKSvQCDg/tapU4e4oBTEUAAAIBg/bd261Zo3b26VKhXfHoqkVARKSLVs2bLYQgMAAOln8+bNLjGF4hFDAQCAcCtWrLAWLVpYcUhKRaAWUsHCKyn41NXA9evXW6NGjUrM/KUTyoQyYT1h22Gfwn7Wb8ce3V6ei1UVG0Olsz179tjbb79tPXr0sCpVqiT65yCOqOv0Qn2nF+o7+vgpGBsUh6RUBMEuewqmSktK7dy5001DUooyYT2JHtsOZcK6Eju2H8rEDzFUup/I1KhRw5UPSSl/o67TC/WdXqjv6JU2JBLNewAAAAAAAOA5klIAAAAAAADwHEkpAAAAAAAAeI6kFAAAAAAAADxHUgoAAAAAAACeIykFAAAAAAAAz5GUAgAAAAAAQHolpcaNG2e/+c1vrHbt2ta4cWPr27evLV68uNTPvfzyy3booYda9erV7YgjjrA33nijwPuBQMBGjhxpzZo1s6ysLOvevbv98MMPcVwSAAAAbxA/AQAAv0hoUuqDDz6wa665xj755BN75513bM+ePdajRw/bvn17sZ/5+OOP7cILL7RBgwbZZ5995hJZenz99dehae677z57+OGHbfLkyfbpp59azZo1rWfPnrZz506PlgwAACA+iJ8AAIBfVE7kl8+cObPA82nTprkWUwsWLLATTjgh4mcmTpxop59+ut18883u+ZgxY1xC69FHH3VJKLWSmjBhgt1xxx3Wp08fN8306dOtSZMmNmPGDLvgggs8WDIAAID4IH4CAAB+kVRjSm3evNn9rV+/frHTzJ0713XHC6dWUHpdcnJybM2aNQWmqVu3rnXt2jU0DQAAgF8QPwEAgFSV0JZS4fLz8+3666+3Y4891tq3b1/sdEo4qdVTOD3X68H3g68VN01hu3btco+gLVu2hH6THiX9ZrXMKmma4mzYsCH0PX6i8ti6dat7ZGRkWLKpU6eONWzY0NPvLM964leUCWXi53Ul3vv3ZN/PJoLKJC8vL27792Re/xIZP5UnhoqVH+Inravy448/WmZmpqUqL2KqVK9vv9S1UN+lo77TS/AYF6/jnR9EWy5Jk5TS2FIaF2rOnDkJGTB09OjRRV5fv359ieNQqZB1dVLBcKVK0Tc602cefvQxy92923wnI8OaNGpoa9dv0FmCJZusqlXtuiFXu9ZzXol1PfEzyoQy8eu64sn+Pcn3swmRkWGtmjW1C/qdb/Xq1avw2SsBmKwSGT+VJ4aKdft6bOJE252ba6mscuXK1v3MM+3RBx6wvXv3WqqqmpVlVw8dGreYyg/17Ze6Fuq7dNR3eglu0zreqe4Re/yUFKU3ZMgQ+9e//mUffvihtWjRosRpmzZtamvXri3wmp7r9eD7wdd0973waTp16hRxnsOHD7dhw4aFnuuKTMuWLa1Ro0buqkBJJ0u6Sq3pynKytG3bNvt2yVI78eIrrEHTkpc35QQClpWXa80zs9xJQjLZuGalffDsE+5KlcYu80qs64mfUSaUiV/XFU/270m8n02UjWtW2IqPZsZt/667/SajRMdP5YmhYt2+ln77rV19/PHWokEDS1X5GRm2zMwGtW9vlVI0sbxy40Z77KOP4hpT+aG+/VDXQn1Hh/pOL7pJm+h4V6VKlUT/nKQUbfyU0KSUrnxfe+219uqrr9rs2bMtOzu71M9069bNZs2a5ZqqB2mgc70umocCK00TDKIUIOkufFdddVXEeVarVs09CtMJUGknQTpZima6wp/Rsjdo1tKaHlD6MqeUQL5lbN1kdWvXN8tIshPIfeUerDNvv7rs64nfUSaUiR/XFU/278m8n02g5XHcvyfbupcs8VN5Y6hYt6+WDRrYgYW6GaYSXVtXoiK7cePkuDocA6XD4x1T+aG+/VDXQn1Hh/pOL8F9X6rEqIkQbblUTnST8+eee85ee+01q127dmjMAjUDzsrKcv/v37+/7b///q55uAwdOtROPPFEe/DBB6137972wgsv2Pz58+3JJ58MHcAUcI0dO9YOPvhgF2SNGDHCmjdvbn379k3g0gIAAJQf8RMAAPCLhCalHn/8cff3pJNOKvD61KlT7ZJLLnH/X758eYEM2zHHHOMSWXfccYfdfvvtLvE0Y8aMAoN73nLLLbZ9+3a7/PLL7ddff7XjjjvO3T45WZvfAwAARIv4CQAA+EXCu++VRs3SCzvvvPPcozhqLXXXXXe5BwAAgJ8QPwEAAL+g8yMAAAAAAAA8R1IKAAAAAAAAniMpBQAAAAAAAM+RlAIAAAAAAIDnSEoBAAAAAADAcySlAAAAAAAA4DmSUgAAAAAAAPAcSSkAAAAAAAB4jqQUAAAAAAAAPEdSCgAAAAAAAJ4jKQUAAAAAAADPkZQCAAAAAACA50hKAQAAAAAAwHMkpQAAAAAAAOA5klIAAAAAAADwHEkpAAAAAAAAeI6kFAAAAAAAADxHUgoAAAAAAACeIykFAAAAAAAAz5GUAgAAAAAAgOdISgEAAAAAAMBzJKUAAAAAAADgOZJSAAAAAAAA8BxJKQAAAAAAAHiOpBQAAAAAAAA8R1IKAAAAAAAA6ZWU+vDDD+2ss86y5s2bW0ZGhs2YMaPE6S+55BI3XeFHu3btQtPceeedRd4/9NBDPVgaAAAAbxBDAQAAP0hoUmr79u3WsWNHmzRpUlTTT5w40VavXh16rFixwurXr2/nnXdegemUpAqfbs6cOXFaAgAAAO8RQwEAAD+onMgv79Wrl3tEq27duu4RpJZVv/zyiw0cOLDAdJUrV7amTZtW6G8FAABIFsRQAADADxKalCqvp59+2rp3724HHHBAgdd/+OEH1yWwevXq1q1bNxs3bpy1atWq2Pns2rXLPYK2bNni/ubn57tHcfReIBAocZpI9Bl1K7RAwCxQts8mPbdM+x6WZMu2r9xjqbPyiHU98TPKhDLx67riyf49mfezPt2/p8r6l0oxVHm2r1Rf8/PD/qbqcqgO4h1T+aG+/VDXQn1Hh/pOL8F9XzyOd34RbbmkbFJq1apV9uabb9pzzz1X4PWuXbvatGnTrG3btq7r3ujRo+3444+3r7/+2mrXrh1xXgq4NF1h69evt507d5ZYyJs3b3YHzUqVou8JuXXrVstu1dKy8nItY+sm85eAZeRuNcvQ/90/SUPlrXJX+a9bt86z7411PfEzyoQy8eu64s3+PXn3s4mi8m7cqKFt27YtLvt31aufJEMMFWs9tMzOth1ZWbYxM9NSVd6+v5syMy1Vl0J1oLqIZ0zlh/r2Q10L9R0d6ju97N27N3S8U08txB4/pWzpPfPMM1avXj3r27dvgdfDuwN26NDBBVi6CvjSSy/ZoEGDIs5r+PDhNmzYsAJX+Vq2bGmNGjWyOnXqlHiypCs4mq4sJ0sKmnOWr7AjM7Osbu365ivu6r1ZoFZ9XUKzZJK7aYsrdwXWjRs39ux7Y11P/IwyoUz8uq54sn9P4v1souRu3Gzr1m+wWrVqxWX/rlZDfpIMMVSs29eKnByr0bGjNajgeXvpv6cxZvXz8lI2EN+Sm+vqIp4xlR/q2w91LdR3dKjv9LJnzx73V8e7KlWqJPrnJKVo46eU3D/qivmUKVPsj3/8o1WtWrXEaRV0HXLIIbZkyZJip6lWrZp7FKYToNJOgnSyFM10hT+jZXAnExnJf5JVNvn7lisJl21fuQfrzNuvLvt64neUCWXix3XFm/17Eu9nfbp/T4V1LxVjqFi3L6ViU7lGKoX9TdXlUB3EO6byQ337oa6F+o4O9Z1egvu+VIlREyHacknJ0vvggw9cgFTcVbvCV1mWLl1qzZo18+S3AQAAJCtiKAAAkEwSmpRSwujzzz93D8nJyXH/X758eahJeP/+/SMOzqkm5e3bty/y3k033eQCrp9++sk+/vhjO/vssy0zM9MuvPBCD5YIAAAg/oihAACAHyS0+978+fPt5JNPDj0PjkkwYMAAN9CmBtkMJqiCNODtP/7xD5s4cWLEea5cudIloDZu3Oj6dx533HH2ySefuP8DAAD4ATEUAADwg4QmpU466aT/jr1RDCWmCqtbt67t2LGj2M+88MILFfb7AAAAkhExFAAA8IOUHFMKAAAAAAAAqY2kFAAAAAAAADxHUgoAAAAAAACeIykFAAAAAAAAz5GUAgAAAAAAgOdISgEAAAAAAMBzJKUAAAAAAADgOZJSAAAAAAAA8BxJKQAAAAAAAHiOpBQAAAAAAAA8R1IKAAAAAAAAniMpBQAAAAAAAM+RlAIAAAAAAIDnSEoBAAAAAADAcySlAAAAAAAA4DmSUgAAAAAAAPAcSSkAAAAAAAB4jqQUAAAAAAAAPEdSCgAAAAAAAJ4jKQUAAAAAAADPkZQCAAAAAACA50hKAQAAAAAAwHMkpQAAAAAAAOA5klIAAAAAAADwHEkpAAAAAAAApFdS6sMPP7SzzjrLmjdvbhkZGTZjxowSp589e7abrvBjzZo1BaabNGmStW7d2qpXr25du3a1efPmxXlJAAAAvEMMBQAA/CChSant27dbx44dXRKpLBYvXmyrV68OPRo3bhx678UXX7Rhw4bZqFGjbOHChW7+PXv2tHXr1sVhCQAAALxHDAUAAPygciK/vFevXu5RVkpC1atXL+J748ePt8GDB9vAgQPd88mTJ9vrr79uU6ZMsdtuu63cvxkAACDRiKEAAIAfJDQpFatOnTrZrl27rH379nbnnXfascce617fvXu3LViwwIYPHx6atlKlSta9e3ebO3dusfPTvPQI2rJli/ubn5/vHsXRe4FAoMRpItFn1O3QAgGzQNk+m/TcMu17WJIt275yj6XOyiPW9cTPKBPKxK/riif792Tez/p0/54q618qxVDl2b5Sfc3PD/ubqsuhOoh3TOWH+vZDXQv1HR3qO70E933xON75RbTlklJJqWbNmrmWT0cffbQLgP7yl7/YSSedZJ9++qkdddRRtmHDBsvLy7MmTZoU+JyeL1q0qNj5jhs3zkaPHl3k9fXr19vOnTtLLOTNmze7g6YCt2ht3brVslu1tKy8XMvYusn8JWAZuVvNMvR/90/SUHmr3FX+XnbnjHU98TPKhDLx67rizf49efeziaLybtyooW3bti0u+3fVa6pLthgq1npomZ1tO7KybGNmpqWqvH1/N2VmWqouhepAdRHPmMoP9e2HuhbqOzrUd3rZu3dv6HhXuXJKpVU8E238lFKl17ZtW/cIOuaYY2zp0qX20EMP2V//+teY56urghqHKvwqX8uWLa1Ro0ZWp06dEk+WdAVH05XlZElBc87yFXZkZpbVrV3ffMVdvTcL1KqvS2iWTHI3bXHlXrt27QLjkMVbrOuJn1EmlIlf1xVP9u9JvJ9NlNyNm23d+g1Wq1atuOzfdeOUVJdsMVSs29eKnByr0bGjNajgeXvpv6cxZvXz8lIrEA+zJTfX1UU8Yyo/1Lcf6lqo7+hQ3+llz5497q+Od1WqVEn0z0lK0cZPqbx/dLp06WJz5sxx/2/YsKFlZmba2rVrC0yj502bNi12HtWqVXOPwnQCVNpJkE6Wopmu8Gd01d+dTGQk/0lW2eTvW64kXLZ95R6sM2+/uuzrid9RJpSJH9cVb/bvSbyf9en+PRXWvVSMoWLdvpSKTeUaqRT2N1WXQ3UQ75jKD/Xth7oW6js61Hd6Ce77UiVGTYRoyyXlS+/zzz93TdKlatWq1rlzZ5s1a1aBq+x63q1btwT+SgAAgORCDAUAABItoS2l1Cx3yZIloec5OTkuQKpfv761atXKNQn/+eefbfr06e79CRMmWHZ2trVr186NU6DxEN577z17++23Q/NQE/IBAwa4MRN0BVCf0W2Tg3fjAwAASHXEUAAAwA8SmpSaP3++nXzyyaHnwTEJlFSaNm2arV692pYvXx56X3eGufHGG12iqkaNGtahQwd79913C8yjX79+brCxkSNH2po1a9xdZmbOnFlk4E4AAIBURQwFAAD8IKFJKd31xY29UQwlpsLdcsst7lGaIUOGuAcAAIAfEUMBAAA/SPkxpQAAAAAAAJB6SEoBAAAAAADAcySlAAAAAAAA4DmSUgAAAAAAAPAcSSkAAAAAAAB4jqQUAAAAAAAAPEdSCgAAAAAAAJ4jKQUAAAAAAADPkZQCAAAAAACA50hKAQAAAAAAwHMkpQAAAAAAAOA5klIAAAAAAADwHEkpAAAAAAAAeI6kFAAAAAAAADxHUgoAAAAAAACeIykFAAAAAAAAz5GUAgAAAAAAgOdISgEAAAAAAMBzJKUAAAAAAADgOZJSAAAAAAAA8BxJKQAAAAAAAHiOpBQAAAAAAAA8R1IKAAAAAAAAniMpBQAAAAAAAM+RlAIAAAAAAEBqJKV+/PHHCvnyDz/80M466yxr3ry5ZWRk2IwZM0qc/pVXXrHTTjvNGjVqZHXq1LFu3brZW2+9VWCaO++8080r/HHooYdWyO8FAAAoD2IoAACAcial2rRpYyeffLI9++yztnPnTovV9u3brWPHjjZp0qSok1hKSr3xxhu2YMEC9xuU1Prss88KTNeuXTtbvXp16DFnzpyYfyMAAEBFIYYCAAD4n8oWg4ULF9rUqVNt2LBhNmTIEOvXr58NGjTIunTpUqb59OrVyz2iNWHChALP77nnHnvttdfsn//8px155JGh1ytXrmxNmzYt028BAACIN2IoAACAcraU6tSpk02cONFWrVplU6ZMca2RjjvuOGvfvr2NHz/e1q9fb17Iz8+3rVu3Wv369Qu8/sMPP7gugQceeKBddNFFtnz5ck9+DwAAQEmIoQAAAMrZUir04cqV7ZxzzrHevXvbY489ZsOHD7ebbrrJbr/9djv//PPt3nvvtWbNmlm8PPDAA7Zt2zb3XUFdu3a1adOmWdu2bV2ybPTo0Xb88cfb119/bbVr1444n127drlH0JYtW0JJLz2Ko/cCgUCJ00Siz2isKwsEzAJl+2zSc8u072FJtmz7yj2WOiuPWNcTP6NMKBO/riue7N+TeT/r0/17POaZ7jFUebavVF/z88P+pupyqA7iHVP5ob79UNdCfUeH+k4vwX1fPI53fhFtuZQrKTV//nzXUuqFF16wmjVrumBK3fhWrlzpApk+ffrYvHnzLB6ee+459x3qvte4cePQ6+HdATt06OACrAMOOMBeeukl99siGTdunJtXYWrxVdKYWSrkzZs3u4NmpUrRNzpT667sVi0tKy/XMrZuMn8JWEbuVrMM/d/9kzRU3ip3lf+6des8+95Y1xM/o0woE7+uK97s35N3P5soKu/GjRq6JEs89u+q14qW7jFUrPXQMjvbdmRl2cbMTEtVefv+bsrMtFRdCtWB6iKeMZUf6tsPdS3Ud3So7/Syd+/e0PFOF5oQe/wUU+mpi57GlFq8eLGdccYZNn36dPc3eMKQnZ3trrS1bt3a4kEB3GWXXWYvv/yyde/evcRp69WrZ4cccogtWbKk2Gl0dVLjY4Vf5WvZsmXoLn8lnSzpCo6mK8vJkoLmnOUr7MjMLKtbu2DXw5Tnrt6bBWrV1yU0Sya5m7a4ctfV3vAgPN5iXU/8jDKhTPy6rniyf0/i/Wyi5G7cbOvWb7BatWrFZf9evXr1CpsXMVT5tq8VOTlWo2NHa1BCfJbs/nsaY1Y/L698V4cTaEturquLeMZUfqhvP9S1UN/Rob7Ty549e9xfxahVqlRJ9M9JStHGTzHtHx9//HG79NJL7ZJLLim2abkOUE8//bRVtOeff959txJTavIezQFt6dKl9sc//rHYaapVq+YehekEqLSTIJ0sRTNd4c/oqr87mchI/pOsssnft1xJuGz7yj1YZ95+ddnXE7+jTCgTP64r3uzfk3g/69P9e0XOkxiq/NuXUrGpvOZXCvubqsuhOoh3TOWH+vZDXQv1HR3qO70E932pEqMmQrTlElNSSgOJl6Zq1ao2YMCAUhNG4S2YcnJy7PPPP3cDl7dq1cq1YPr5559dS6xgc3PNU4Osq0n5mjVr3OtZWVlWt25d9381fz/rrLNcc3MNxD5q1CjLzMy0Cy+8MJZFBQAAqDDEUAAAAP8TU0pPXffUda4wvfbMM8+UaTyFI4880j1EXej0/5EjR7rnGmQz/M55Tz75pOu7ec0117gWWsHH0KFDQ9NoLAYloDRIpwbvbNCggX3yySeuWR0AAEAiEUMBAACUs6WUBrV84oknInbZu/zyy0ttIRV00kkn/bebQzE0LlW42bNnlzpPdesDAABIRsRQAAAA5WwppdZLGsy8MHWZC2/ZBAAAAGIoAACACktKqUXUl19+WeT1L774wnWXAwAAADEUAABAhSelNGbTddddZ++//77l5eW5x3vvvefGdrrgggtimSUAAIDvEUMBAACUc0ypMWPG2E8//WSnnnqqVa7831nk5+db//797Z577olllgAAAL5HDAUAAFDOpFTVqlXtxRdfdIGVuuxlZWXZEUcc4caUAgAAADEUAABAXJJSQYcccoh7AAAAgBgKAAAg7kkpjSE1bdo0mzVrlq1bt8513Qun8aUAAABADAUAAFChSSkNaK6kVO/eva19+/aWkZERy2wAAADSCjEUAABAOZNSL7zwgr300kt2xhlnxPJxAACAtEQMBQAA8D+VLMaBztu0aRPLRwEAANIWMRQAAEA5k1I33nijTZw40QKBQCwfBwAASEvEUAAAAOXsvjdnzhx7//337c0337R27dpZlSpVCrz/yiuvxDJbAAAAXyOGAgAAKGdSql69enb22WfH8lEAAIC0RQwFAABQzqTU1KlTY/kYAABAWiOGAgAAKOeYUrJ3715799137YknnrCtW7e611atWmXbtm2LdZYAAAC+RwwFAABQjpZSy5Yts9NPP92WL19uu3btstNOO81q165t9957r3s+efLkWGYLAADga8RQAAAA5WwpNXToUDv66KPtl19+saysrNDrGmdq1qxZscwSAADA94ihAAAAytlS6qOPPrKPP/7YqlatWuD11q1b288//xzLLAEAAHyPGAoAAKCcLaXy8/MtLy+vyOsrV6503fgAAABADAUAAFDhSakePXrYhAkTQs8zMjLcAOejRo2yM844I5ZZAgAA+B4xFAAAQDm77z344IPWs2dPO/zww23nzp32hz/8wX744Qdr2LChPf/887HMEgAAwPeIoQAAAMqZlGrRooV98cUX9sILL9iXX37pWkkNGjTILrroogIDnwMAAIAYCgAAoMKSUu6DlSvbxRdfHOvHAQAA0hIxFAAAQDmSUtOnTy/x/f79+8cyWwAAAF8jhgIAAChnUmro0KEFnu/Zs8d27NhhVatWtRo1apCUAgAAIIYCAACo+Lvv/fLLLwUeGlNq8eLFdtxxx5VpoPMPP/zQzjrrLGvevLm7g9+MGTNK/czs2bPtqKOOsmrVqlmbNm1s2rRpRaaZNGmStW7d2qpXr25du3a1efPmlXkZAQAAKhoxFAAAQDmTUpEcfPDB9uc//7lIK6qSbN++3Tp27OiSSNHIycmx3r1728knn2yff/65XX/99XbZZZfZW2+9FZrmxRdftGHDhtmoUaNs4cKFbv66U+C6detiWi4AAIB4IoYCAADpqnKFzqxyZVu1alXU0/fq1cs9ojV58mTLzs52t1OWww47zObMmWMPPfSQSzzJ+PHjbfDgwTZw4MDQZ15//XWbMmWK3XbbbWVeJgAAgHgjhgIAAOkopqTU//3f/xV4HggEbPXq1fboo4/asccea/Eyd+5c6969e4HXlIxSiynZvXu3LViwwIYPHx56v1KlSu4z+iwAAEAiEUMBAACUMynVt2/fAs81HlSjRo3slFNOCbViioc1a9ZYkyZNCrym51u2bLHc3Fw3TkNeXl7EaRYtWlTsfHft2uUeQZqf5Ofnu0dx9J4SciVNE4k+ozKzQMAsULbPJj23TPselmTLFgjYnj277aeffnJ14N3XBmzr1q3u4eo9DSlhrBshBFEmRcWjTOrUqWMNGza0VBbrfjZRPNm/J/N+NlH2lXu81pWKnCcxVPm3r1Rf8/PD/qbqcqgO4rnN+aW+/VDXQn1Hh/qO3oYNG0Ln3KlKeQf58ccfLTMz01JVnTieL0R7fIgpKZUqJwfRGjdunI0ePbrI6+vXr7edO3eWWA6bN292B021yIqWTjqzW7W0rLxcy9i6yfwlYBm5W83cOXVyJWACm9dbzerVbdrfnrcqVap498UZGdakUUNbu37DvpPI9LJn7x7bsG6dNW7S9H877DQvk4jiUCZZVavadUOutrp161qqinU/myje7N+Tdz+bKCrvxo0auhuvxGMMSdVrRSGGKl89tMzOth1ZWbYxhU8A/nsaY7YpM9NSdSlUB6oL1Um8xm31Q337oa6F+o4O9R0dxXWPTZxou3NzLdW73Xc/80x79IEHbO/evZaqqmZl2dVDh8blfCHa+KlCx5SKt6ZNm9ratWsLvKbnyu5lZWW5E149Ik2jzxZH3f00OHqQsrYtW7Z0rb8075ICy2ArsbKcLClozlm+wo7MzLK6teubr7ir92aBWvXdSXYy2bDjG/vuhyV21LmX2P4HHOjdFwcC7oSpeWZW0pWJF5Z8Md9mfzTeLjp7wP/KPc3LJKIKLpONa1baB88+4faJjRs3tlQV6342UTzZvyfxfjZRcjdutnXrN1itWrXisr7rbr6pLtliqFi3rxU5OVajY0drUMHz9lLw1KV+Xl5qBeJhtuTmurqoXbt23I4xfqhvP9S1UN/Rob6j37aXfvutXX388daiQQNLVfkZGbbMzAa1b2+VUvQi+8qNG+2xjz6K2/lCtPFTTPvH8OCjNBp4vKJ069bN3njjjQKvvfPOO+51Ufegzp0726xZs0LN43VCo+dDhgwpdr7VqlVzj8J0AlTaSZBOlqKZrvBnXPcxnUxkJP9JVtnk71uuZFy2DLc+NGja3Jq29jIple9aTLgT1KQrk/hbv3pl0XJP8zKJqKLLZN9+JriPSmWx7GcTxZv9ezLvZxMkzut7Rc6TGKr825dSsam85lcK+5uqy6E6iPcxxg/17Ye6Fuo7OtR32bbtlg0a2IGFht1JtSSkklLZjRunbNI5I8778mjnGVP5ffbZZ+6xZ88ea9u2rXvt+++/dxm2o446KjRdaeOiKEu6ZMmS0POcnBz7/PPPrX79+taqVSt39e3nn3+26dOnu/evvPJKN5j6LbfcYpdeeqm999579tJLL7m764UHewMGDLCjjz7aunTpYhMmTLDt27eH7sYHAACQKMRQAAAA5UxKnXXWWa657jPPPGP77befe02DjCvxc/zxx9uNN94Y1Xzmz59vJ598cpGrh0oqTZs2zd3Rb/ny5aH3s7OzXQLqhhtusIkTJ1qLFi3sL3/5i7sDX1C/fv3cWFAjR450A6N36tTJZs6cWWTwcwAAAK8RQwEAAJQzKaU77L399tuhhJTo/2PHjrUePXpEnZQ66aSTSrwLmhJTkT6jq4wlUVe9krrrAQAAJAIxFAAAwP/E1HFQg1iqNVJheq0i71ADAADgJ8RQAAAA5UxKnX322a6r3iuvvGIrV650j3/84x82aNAgO+ecc2KZJQAAgO8RQwEAAJSz+97kyZPtpptusj/84Q9usHM3o8qVXVLq/vvvj2WWAAAAvkcMBQAAUM6kVI0aNeyxxx5zCailS5e61w466CCrWbNmLLMDAABIC8RQAAAA5ey+F6S74+lx8MEHu4RUSYOWAwAAgBgKAACgXEmpjRs32qmnnmqHHHKInXHGGS4xJeq+F+2d9wAAANINMRQAAEA5k1I33HCDValSxZYvX+6aoQf169fPZs6cGcssAQAAfI8YCgAAoJxjSr399tv21ltvWYsWLQq8rm58y5Yti2WWAAAAvkcMBQAAUM6WUtu3by/QQipo06ZNVq1atVhmCQAA4HvEUAAAAOVMSh1//PE2ffr00POMjAzLz8+3++67z04++eRYZgkAAOB7xFAAAADl7L6n5JMGOp8/f77t3r3bbrnlFvvmm29cS6l///vfscwSAADA94ihAAAAytlSqn379vb999/bcccdZ3369HFN0c855xz77LPP7KCDDopllgAAAL5HDAUAAFCOllJ79uyx008/3SZPnmx/+tOfyvpxAACAtEQMBQAAUM6WUlWqVLEvv/yyrB8DAABIa8RQAAAAFdB97+KLL7ann346lo8CAACkLWIoAACAcg50vnfvXpsyZYq9++671rlzZ6tZs2aB98ePHx/LbAEAAHyNGAoAACDGpNSPP/5orVu3tq+//tqOOuoo95oGPA+XkZFRllkCAAD4HjEUAABAOZNSBx98sK1evdref/9997xfv3728MMPW5MmTcoyGwAAgLRCDAUAAFDOMaUCgUCB52+++aZt3769LLMAAABIO8RQAAAAFTTQeXEBFgAAAIihAAAAKjwppfGiCo8ZxRhSAAAAxFAAAABxHVNKLaMuueQSq1atmnu+c+dOu/LKK4vcfe+VV14p8w8BAADwK2IoAACAcialBgwYUOD5xRdfXJaPAwAApCViKAAAgHImpaZOnVqWyQEAAEAMBQAAUPEDnQMAAAAAAACxICkFAAAAAACA9ExKTZo0yVq3bm3Vq1e3rl272rx584qd9qSTTgrdBTD80bt379A0Goy98Punn366R0sDAAAQf8RPAAAgrcaUiocXX3zRhg0bZpMnT3YJqQkTJljPnj1t8eLF1rhx4yLT685+u3fvDj3fuHGjdezY0c4777wC0ykJFT4GVvCOgQAAAKmO+AkAAPhBwltKjR8/3gYPHmwDBw60ww8/3CWnatSoYVOmTIk4ff369a1p06ahxzvvvOOmL5yUUhIqfLr99tvPoyUCAACIL+InAADgBwltKaUWTwsWLLDhw4eHXqtUqZJ1797d5s6dG9U8nn76abvgggusZs2aBV6fPXu2a2mlZNQpp5xiY8eOtQYNGkScx65du9wjaMuWLe5vfn6+exRH7wUCgRKniUSfUZdCCwTMAmX7bNJzy7TvYcm2bAG3fnle7kldJgkq97QvEw/Wk337mVj2Uckk1v1soniyf2f78Xx9T7b1L1nip/LEUOXZvlL9aJof9jdVl0N1EO9jjB/q2w91LdR3dKjv9Nm2/VLfgTjvy6OdZ0KTUhs2bLC8vDxr0qRJgdf1fNGiRaV+XmNPff311y6wKtx175xzzrHs7GxbunSp3X777darVy8XqGVmZhaZz7hx42z06NFFXl+/fr3t3LmzxELevHmzq0R30h2lrVu3WnarlpaVl2sZWzeZvwQsI3erWYb+7/5JGnUzzdod2tZqBnZ7XO7JWyaJK/f0LpPIKrZMtH/Rfkb7m3Xr1lmqinU/myje7N/ZfgpTeTdu1NC2bdsWl/Vd9ZpMkiV+Kk8MFWs9tMzOth1ZWbaxmN+TCvL2/d2UmWmpuhSqA9VFPI8xfqhvP9S1UN/Rob7TZ9v2S33viPO+PNr4KeFjSpWHgqkjjjjCunTpUuB1XfkL0vsdOnSwgw46yF39O/XUU4vMR1caNa5V+FW+li1bWqNGjaxOnTolniwps6jpynKypKA5Z/kKOzIzy+rWrm++4q7gmwVq1Vfa1ZLJ5jyzbxYttlMzqlrAy3JP4jJJWLmneZlEVMFlkrtpi9vP1K5dO+L4fKki1v1soniyf2f7KSJ342Zbt36D1apVKy7ru27E4icVFT+VJ4aKdftakZNjNTp2tAYVPG8v7d33t35eXsoG4ltyc11dxPMY44f69kNdC/UdHeo7fbZtv9T3ljjvy6ONnxJafg0bNnRX3tauXVvgdT3XOFAl2b59u73wwgt21113lfo9Bx54oPuuJUuWRAyqNP5UpIHQdQJU2kmQTpaima7wZ3TV3514ZiT/SVbZ5O9brmRctoz/NiH0/Lclc5kkqtzTvUwiqeAy2befCe6jUlks+9lE8Wb/zvbj9fqebOtessRP5Y2hYt2+lLZPrhopm0phf1N1OVQH8T7G+KG+/VDXQn1Hh/pOn23bL/WdEed9ebTzTGj5Va1a1Tp37myzZs0KvaaTVz3v1q1biZ99+eWX3RgGF198canfs3LlSneXvmbNmlXI7wYAAEgU4icAAOAXCU/qqcn3U089Zc8884x99913dtVVV7mreLobn/Tv37/AQJ7hTc/79u1bZPBNNQe8+eab7ZNPPrGffvrJJbj69Oljbdq0sZ49e3q2XAAAAPFC/AQAAPwg4d0f+/Xr5wbDHDlypK1Zs8Y6depkM2fODA3euXz58iLNvhYvXmxz5syxt99+u8j81Jz9yy+/dEmuX3/91Zo3b249evSwMWPGRGxeDgAAkGqInwAAgB8kPCklQ4YMcY9INLhmYW3btv3vmB0RZGVl2VtvvVXhvxEAACCZED8BAIBUl/DuewAAAAAAAEg/JKUAAAAAAADgOZJSAAAAAAAA8BxJKQAAAAAAAHiOpBQAAAAAAAA8R1IKAAAAAAAAniMpBQAAAAAAAM+RlAIAAAAAAIDnSEoBAAAAAADAcySlAAAAAAAA4DmSUgAAAAAAAPAcSSkAAAAAAAB4jqQUAAAAAAAAPEdSCgAAAAAAAJ4jKQUAAAAAAADPkZQCAAAAAACA50hKAQAAAAAAwHMkpQAAAAAAAOA5klIAAAAAAADwHEkpAAAAAAAAeI6kFAAAAAAAADxHUgoAAAAAAACeIykFAAAAAAAAz5GUAgAAAAAAgOdISgEAAAAAACA9k1KTJk2y1q1bW/Xq1a1r1642b968YqedNm2aZWRkFHjoc+ECgYCNHDnSmjVrZllZWda9e3f74YcfPFgSAAAAbxA/AQCAVJfwpNSLL75ow4YNs1GjRtnChQutY8eO1rNnT1u3bl2xn6lTp46tXr069Fi2bFmB9++77z57+OGHbfLkyfbpp59azZo13Tx37tzpwRIBAADEF/ETAADwg4QnpcaPH2+DBw+2gQMH2uGHH+4SSTVq1LApU6YU+xm1jmratGno0aRJkwKtpCZMmGB33HGH9enTxzp06GDTp0+3VatW2YwZMzxaKgAAgPghfgIAAH5QOZFfvnv3bluwYIENHz489FqlSpVcd7u5c+cW+7lt27bZAQccYPn5+XbUUUfZPffcY+3atXPv5eTk2Jo1a9w8gurWreu6BWqeF1xwQZH57dq1yz2CtmzZ4v5q/noUR+8pCVbSNJHoM0qsWSBgFijbZ5OeW6Z9D0u2ZQu49cvzck/qMklQuad9mXiwnuzbz8Syj0omse5nE8WT/Tvbj+fre7Ktf8kSP5UnhirP9pXqR9P8sL+puhyqg3gfY/xQ336oa6G+o0N9p8+27Zf6DsR5Xx7tPBOalNqwYYPl5eUVaOkker5o0aKIn2nbtq1rRaUWUJs3b7YHHnjAjjnmGPvmm2+sRYsWLqAKzqPwPIPvFTZu3DgbPXp0kdfXr19fYpc/FbJ+gyrRnXRHaevWrZbdqqVl5eVaxtZN5i8By8jdapah/7t/kkbdTLN2h7a1moHdHpd78pZJ4so9vcsksootE+1ftJ/R/qak7tDJLtb9bKJ4s39n+ylM5d24UUOXdInH+q56TSbJEj+VJ4aKtR5aZmfbjqws25iZaakqb9/fTZmZlqpLoTpQXcTzGOOH+vZDXQv1HR3qO322bb/U944478ujjZ8SmpSKRbdu3dwjSAHVYYcdZk888YSNGTMmpnnqSqPGtQq/yteyZUtr1KiRG7+qpJMlZRY1XVlOlhQ05yxfYUdmZlnd2vXNV9wVfLNArfpKu1oy2Zxn9s2ixXZqRlULeFnuSVwmCSv3NC+TiCq4THI3bXH7mdq1a1vjxo0tVcW6n00UT/bvbD9F5G7cbOvWb7BatWrFZX0vfEOVVBSP+Kk8MVSs29eKnByr0bGjNajgeXtp776/9fPyUi8Q32dLbq6ri3geY/xQ336oa6G+o0N9p8+27Zf63hLnfXm08VNCy69hw4aWmZlpa9euLfC6nmusqGhUqVLFjjzySFuyZIl7Hvyc5qG774XPs1OnThHnUa1aNfcoTCdApZ0E6WQpmukKf0ZX/d2JZ0byn2SVTf6+5UrGZcv4bxNCz39bMpdJoso93cskkgouk337meA+KpXFsp9NFG/272w/Xq/vybbuJUv8VN4YKtbtS2n75KqRsqkU9jdVl0N1EO9jjB/q2w91LdR3dKjv9Nm2/VLfGXHel0c7z4SWX9WqVa1z5842a9as0Gs6edXz8Kt5JVHz9a+++ioUQGVnZ7vAKnyeumqnu/BFO08AAIBkRfwEAAD8IuEtzdTke8CAAXb00Udbly5d3J3ztm/f7u7GJ/3797f999/fjVkgd911l/32t7+1Nm3a2K+//mr333+/LVu2zC677DL3vrJ8119/vY0dO9YOPvhgl6QaMWKENW/e3Pr27ZvQZQUAAKgIxE8AAMAPEp6U6tevnxsMc+TIkW4gTTURnzlzZmigzeXLlxdo9vXLL7/Y4MGD3bT77befa2n18ccf2+GHHx6a5pZbbnGJrcsvv9wlro477jg3Tz+MCQEAAED8BAAA/CDhSSkZMmSIe0Qye/bsAs8feugh9yiJWkupRZUeAAAAfkT8BAAAUl2qjskFAAAAAACAFEZSCgAAAAAAAJ4jKQUAAAAAAADPkZQCAAAAAACA50hKAQAAAAAAwHMkpQAAAAAAAOA5klIAAAAAAADwHEkpAAAAAAAAeI6kFAAAAAAAADxHUgoAAAAAAACeIykFAAAAAAAAz5GUAgAAAAAAgOdISgEAAAAAAMBzJKUAAAAAAADgOZJSAAAAAAAA8BxJKQAAAAAAAHiOpBQAAAAAAAA8R1IKAAAAAAAAniMpBQAAAAAAAM+RlAIAAAAAAIDnSEoBAAAAAADAcySlAAAAAAAA4DmSUgAAAAAAAPAcSSkAAAAAAAB4jqQUAAAAAAAA0jMpNWnSJGvdurVVr17dunbtavPmzSt22qeeesqOP/5422+//dyje/fuRaa/5JJLLCMjo8Dj9NNP92BJAAAAvEH8BAAAUl3Ck1IvvviiDRs2zEaNGmULFy60jh07Ws+ePW3dunURp589e7ZdeOGF9v7779vcuXOtZcuW1qNHD/v5558LTKck1OrVq0OP559/3qMlAgAAiC/iJwAA4AcJT0qNHz/eBg8ebAMHDrTDDz/cJk+ebDVq1LApU6ZEnP5vf/ubXX311dapUyc79NBD7S9/+Yvl5+fbrFmzCkxXrVo1a9q0aeihVlUAAAB+QPwEAAD8oHIiv3z37t22YMECGz58eOi1SpUquS55agUVjR07dtiePXusfv36RVpUNW7c2CWjTjnlFBs7dqw1aNAg4jx27drlHkFbtmxxf5Xs0qM4ei8QCJQ4TST6jLoUWiBgFijbZ5OeW6Z9D0u2ZQu49cvzck/qMklQuad9mXiwnuzbz8Syj0omse5nE8WT/Tvbj+fre7Ktf8kSP5UnhirP9pXqR9P8sL+puhyqg3gfY/xQ336oa6G+o0N9p8+27Zf6DsR5Xx7tPBOalNqwYYPl5eVZkyZNCryu54sWLYpqHrfeeqs1b97cBWLhXffOOeccy87OtqVLl9rtt99uvXr1coFaZmZmkXmMGzfORo8eXeT19evX286dO0ss5M2bN7tKdCfdUdq6datlt2ppWXm5lrF1k/lLwDJyt5pl6P/un6RRN9Os3aFtrWZgt8flnrxlkrhyT+8yiaxiy0T7F+1ntL8prjt0Koh1P5so3uzf2X4KU3k3btTQtm3bFpf1XfWaTJIlfipPDBVrPbTMzrYdWVm2sZjfkwry9v3dlJlpqboUqgPVRTyPMX6obz/UtVDf0aG+02fb9kt974jzvjza+CmhSany+vOf/2wvvPCCu6qnQdKDLrjggtD/jzjiCOvQoYMddNBBbrpTTz21yHx0pVHjWoVf5dNYVY0aNbI6deqUeLKkzKKmK8vJkoLmnOUr7MjMLKtbu+AVypTnruCbBWrVV9rVksnmPLNvFi22UzOqWsDLck/iMklYuad5mURUwWWSu2mL28/Url3btXpIVbHuZxPFk/07208RuRs327r1G6xWrVpxWd/DYww/qKj4qTwxVKzb14qcHKvRsaM1qOB5e2nvvr/18/JSNhDfkpvr6iKexxg/1Lcf6lqo7+hQ3+mzbfulvrfEeV8ebfyU0PJr2LChu/K2du3aAq/rucaBKskDDzzggqp3333XBU0lOfDAA913LVmyJGJQpfGn9ChMJ0ClnQTpZCma6Qp/Rlf93YlnRvKfZJVN/r7lSsZly/hvE0LPf1syl0miyj3dyySSCi6TffuZ4D4qlcWyn00Ub/bvbD9er+/Jtu4lS/xU3hgq1u1LafvkqpGyqRT2N1WXQ3UQ72OMH+rbD3Ut1Hd0qO/02bb9Ut8Zcd6XRzvPhJZf1apVrXPnzgUGKQ8OWt6tW7diP3fffffZmDFjbObMmXb00UeX+j0rV660jRs3WrNmzSrstwMAACQC8RMAAPCLhCf11OT7qaeesmeeeca+++47u+qqq2z79u3ubnzSv3//AgN53nvvvTZixAh3d77WrVvbmjVr3EPNAEV/b775Zvvkk0/sp59+cgmuPn36WJs2baxnz54JW04AAICKQvwEAAD8IOHdH/v16+cGwxw5cqRLLnXq1Mm1gAoO3rl8+fICzb4ef/xxd9eZ3//+9wXmM2rUKLvzzjtdc/Yvv/zSJbl+/fVXN4hnjx49XMuqSM3LAQAAUg3xEwAA8IOEJ6VkyJAh7hGJBtcMp9ZPJcnKyrK33nqrQn8fAABAsiF+AgAAqS7h3fcAAAAAAACQfkhKAQAAAAAAwHMkpQAAAAAAAOA5klIAAAAAAADwHEkpAAAAAAAAeI6kFAAAAAAAADxHUgoAAAAAAACeIykFAAAAAAAAz5GUAgAAAAAAgOdISgEAAAAAAMBzJKUAAAAAAADgOZJSAAAAAAAA8BxJKQAAAAAAAHiOpBQAAAAAAAA8R1IKAAAAAAAAniMpBQAAAAAAAM+RlAIAAAAAAIDnSEoBAAAAAADAcySlAAAAAAAA4DmSUgAAAAAAAPAcSSkAAAAAAAB4jqQUAAAAAAAAPEdSCgAAAAAAAJ4jKQUAAAAAAADPkZQCAAAAAABAeialJk2aZK1bt7bq1atb165dbd68eSVO//LLL9uhhx7qpj/iiCPsjTfeKPB+IBCwkSNHWrNmzSwrK8u6d+9uP/zwQ5yXAgAAwDvETwAAINUlPCn14osv2rBhw2zUqFG2cOFC69ixo/Xs2dPWrVsXcfqPP/7YLrzwQhs0aJB99tln1rdvX/f4+uuvQ9Pcd9999vDDD9vkyZPt008/tZo1a7p57ty508MlAwAAiA/iJwAA4AcJT0qNHz/eBg8ebAMHDrTDDz/cJZJq1KhhU6ZMiTj9xIkT7fTTT7ebb77ZDjvsMBszZowdddRR9uijj4ZaSU2YMMHuuOMO69Onj3Xo0MGmT59uq1atshkzZni8dAAAABWP+AkAAPhBQpNSu3fvtgULFrjudaEfVKmSez537tyIn9Hr4dOLWkEFp8/JybE1a9YUmKZu3bquW2Bx8wQAAEgVxE8AAMAvKifyyzds2GB5eXnWpEmTAq/r+aJFiyJ+RgmnSNPr9eD7wdeKm6awXbt2uUfQ5s2b3d9ff/3V8vPzi/39em/Lli1WtWpVl0yLlj6Tl7fXVi1dbLnbtpqvBAKWlbfTcjN/NsvIsGSybnmO+32rcpZYYO9e7744icskYeWe5mUSUQWXyS9rV9nO3Fz75ptv3D4nlW3dutVWr15tqWDFihW2e9eu+O7f2X6K+GXtz7Znzx63rujYXdGSbRtKlvipPDFUrPWwNy/PFq9aZVtTeEiG/IwM29GqlX21fLlVCgQsFa3atMnVheokHtucX+rbD3Ut1Hd0qO/02bb9Ut+r4rwvD8ZP6s2WtEmpZDFu3DgbPXp0kdcPOOCAuH7vnHffjuv8EdmEIf0pmgSg3BOjz4fvJ+ib09vc2bMS/RPS0rNTn070T0g7iYih3v73v+M2b5TN20cdFfcio76TB/WdXuJd32zb6VPXW7dudb3XkjIp1bBhQ8vMzLS1a9cWeF3PmzZtGvEzer2k6YN/9Zruvhc+TadOnSLOc/jw4W6w9SBd2du0aZM1aNDAMkposaDMX8uWLd2V8Tp16kS1zH5HmVAmrCdsO+xT2M/67dgTvMJXu3ZtSwbJEj+VJ4ZKZ8RK6YO6Ti/Ud3qhvqOLn5SQat68eYnTJTQppW5vnTt3tlmzZrk76AWDGT0fMmRIxM9069bNvX/99deHXnvnnXfc65Kdne0CK00TDKK0wugufFdddVXEeVarVs09wtWrVy/q5VAATFKKMmE9KTu2HcqEdSV2bD/pWybJEj9VRAyVztJlfQV1nW7YttML9V2yklpIJU33PV1dGzBggB199NHWpUsXd+e87du3u7vxSf/+/W3//fd3zcNl6NChduKJJ9qDDz5ovXv3thdeeMHmz59vTz75pHtfV+UUcI0dO9YOPvhgF2SNGDHCZeeCgRsAAEAqI34CAAB+kPCkVL9+/Wz9+vU2cuRIN5Cmrs7NnDkzNNDmcg0cFjaI+DHHHGPPPfec3XHHHXb77be7xNOMGTOsffv2oWluueUWl9i6/PLL3YBdxx13nJtn9erVE7KMAAAAFYn4CQAA+EFGoLSh0FEs3W1GLbg0nkLhpuvpijKhTFhP2HbYp7Cf5dgDECuBuDjdcB6UXqjvikNSCgAAAAAAAJ77X784AAAAAAAAwCMkpQAAAAAAAOA5klIAAABAkpk9e7a7q7Ru2oP00bp1a3c3cgBIFySliqEBzH/zm99Y7dq1rXHjxta3b19bvHhxiYU5bdo0FzyEP/x0x78777yzyPIdeuihJX7m5ZdfdtOoHI444gh74403zG+BQ+Ey0eOaa65Jm3Xkww8/tLPOOsuaN2/ulkd3wwyneyno7prNmjWzrKws6969u/3www+lznfSpEmufFU+Xbt2tXnz5plfymXPnj126623um2iZs2abpr+/fvbqlWrKnwbTKV15ZJLLimyfKeffrqv15XSyiTS/kWP+++/37frSTTH3507d7r9bIMGDaxWrVp27rnn2tq1a0ucb6z7IsALJ510kl1//fUUdhpRTFivXr1E/wwkENt9+lBs1qlTp0T/jKRFUqoYH3zwgQt4P/nkE3vnnXfcSWSPHj1s+/btJRZonTp1bPXq1aHHsmXLzE/atWtXYPnmzJlT7LQff/yxXXjhhTZo0CD77LPP3ImFHl9//bX5xX/+858C5aF1Rc4777y0WUe0TXTs2NElBiK577777OGHH7bJkyfbp59+6pIwPXv2dCeVxXnxxRdt2LBhNmrUKFu4cKGbvz6zbt0680O57Nixwy3XiBEj3N9XXnnFnXT/7ne/q9BtMNXWFVESKnz5nn/++RLnmerrSmllEl4WekyZMsUlmZSE8et6Es3x94YbbrB//vOf7sKHpldC95xzzilxvrHsi4B42717N4UMAEhvAURl3bp1ARXXBx98UOw0U6dODdStW9e3JTpq1KhAx44do57+/PPPD/Tu3bvAa127dg1cccUVAb8aOnRo4KCDDgrk5+en5TqibeTVV18NPVc5NG3aNHD//feHXvv1118D1apVCzz//PPFzqdLly6Ba665JvQ8Ly8v0Lx588C4ceMCfiiXSObNm+emW7ZsWYVtg6lWJgMGDAj06dOnTPPx07oSzXqi8jnllFNKnMZP60mk46/2IVWqVAm8/PLLoWm+++47N83cuXMjziPWfRFQ2Iknnhi49tprAzfffHNgv/32CzRp0sRtc0Hah//ud78L1KxZM1C7du3AeeedF1izZk2R7fOpp54KtG7dOpCRkeH2fVp/wx85OTmB999/3/3/3XffDXTu3DmQlZUV6NatW2DRokVUjEe0n2nfvn2gevXqgfr16wdOPfXUwLZt29yxZvTo0YH9998/ULVqVVenb775Zuhzwbr75ZdfQq999tlnReo2/BFcjw444IDA3XffHRg4cGCgVq1agZYtWwaeeOIJ6twj//znP12svnfv3gL1duutt4amGTRoUOCiiy5y///oo48Cxx13nFtHWrRo4fYPWkeCJk2aFGjTpo073jRu3Dhw7rnnuteL2+6RONqu77nnHrdvVn126NAhFGtofbj00ktD7x1yyCGBCRMmFPi8tuvf/OY3gRo1arh16Jhjjgn89NNP7vyvcF3rNfwPLaWitHnzZve3fv36JU63bds2O+CAA6xly5bWp08f++abb8xP1NVB3UwOPPBAu+iii2z58uXFTjt37lzXPSKcrkrrdb9e7Xz22Wft0ksvdS0Z0nUdCZeTk2Nr1qwpsB7UrVvXdbEqbj1QOS5YsKDAZypVquSe+3XdCe5jtN6U1pS/LNtgqo6hoi5bbdu2tauuuso2btxY7LTptq6oe9rrr7/uWp+Wxk/rSeHjr+pcrafC613dE1u1alVsvceyLwKK88wzz7iWdmpxpxZ4d911l2vVl5+f747rmzZtci349NqPP/5o/fr1K/D5JUuW2D/+8Q/XSvbzzz+3iRMnWrdu3Wzw4MGh1o2KEYL+9Kc/2YMPPmjz58+3ypUruzgD8ad6UIt/lfd3333njk9qkalrCKoz1ckDDzxgX375pYtv1do52i7BxxxzjBs3Krz1/E033RR6X/M++uijXU+Dq6++2h0PSxtGBBXj+OOPt61bt7qyF23LDRs2dPUfpNfU9W7p0qWuhbdaL2s9UOtttUweMmSIm07b7HXXXef2Eaq/mTNn2gknnODeK227R2KGD5g+fbprUa3zM7XKvvjii119a//eokUL10L722+/dcMB3H777fbSSy+5z+7du9f1CDrxxBPduqDY4vLLL3exvY4BN954Y4FW7IWPC+mucqJ/QCrQSqh+/scee6y1b9++2Ol0EqWuFR06dHBBtA5UOuhopdZKnOoUvKv/u5ZTG9Po0aPdjlvd8TT2R2E6AWjSpEmB1/Rcr/uRxoLRYKQaFydd15HCgnVdlvVgw4YNlpeXF/EzixYtMj9S9yGNMaXgVwFqRW2DqUaBnQL+7OxsF+jpYN+rVy93YM/MzLR0X1d0Iqx6Lq2bmp/Wk0jHX+07qlatWiSBW9J+JZZ9EVAcHcPVZVgOPvhge/TRR23WrFnu+VdffeWSoMGTS53g6ERE3f01Vlowoa7XGzVqFJqn1ukaNWpY06ZNi3zf3Xff7U505LbbbrPevXu740aqj0mZ7LT/1Imm9rm6mCgaC1IUv+m4fcEFF7jn9957r73//vsu0VRSF/Xw+lZiXCesker8jDPOcMko0fc89NBDbv7aryO+VC8a+0dJKCUG9VfJCR1LdWFZ8bsSy9omlcTQhZ/geHDaH6ibuN57/PHH3QUhJbDPPPNMd/zVenTkkUeGvqek7R7e2rVrl91zzz327rvvumSh6MKekoxPPPGEq1OtA0GKVRWfKil1/vnn25YtW9y6obo+6KCD3DSHHXZYaHqNf6mLCtR1ZCSloqCxLRTMlzYmh1bg4EosSjZoZdSKPGbMGEt1OjkMD8h04qOdqzbGaK7c+93TTz/tykitE9J1HUHZqcWHDma68qoAJp23wWBwHwz8tYw6sCsgPPXUUy3dKaGt4Le0E1E/rSfRHn8BL2m7CqfB8zWOnVrTKBkV3trh8MMPdwlUvRdMSml7DE9IleX79F2i71PrQMSPxvvTsUfHI7WE0th2v//9791FEo1jp2R5OD3/4osvKuS7w+s8mLhKlbES/UAJCMUeat3y0UcfueSTjqE6FqklpGJ9JaBU32oV87e//S30WcVzuqCi5PRpp53mtnclN3ThTY+zzz7bJaKQXJRo1JivqrNwuogQTCQq4axYTMnG3Nxc915w8HK15lbDBO0rNA+1zFZ8H9xno2R03yuFml/+61//clcnytqSpUqVKm4l1kruRwqyDjnkkGKXTwfQwndD0nM/Zog1WLky65dddlmZPuf3dSRY12VZD9REWgFfOqw7wYSU1h918yiplVQs22CqUxCn9aG45UundUVBsZr+l3Ufk8rrSXHHX9WtAkG1TI223mPZFwElHbvDKWmgk9BoqeVErN8XHB6gLN+H2Oj4omPzm2++6ZKLjzzyiGuppGRDadSVXP47XOD/jvlerWMoH3XNUwJKSSfVhbqI6zUlqtSVK9hyUS2nrrjiCtcNN/jQZ9SNUxfV1DpKN2HRTVuUnFCXLyU7Cx+/kHiqS9EwCeH1qa56f//73+2FF15wXWx1ce/tt9927w0cOLDAzSqmTp3qWk+p0YG6cir20k1bUDqSUsXQQUQB8auvvmrvvfeea6JXVupWombcfs2QauNVF5vilk8tgoLN2YN0cA9vKeQX2glpHBw1qS8Lv68j2m50whe+Hqh5q8bhKG49UFPmzp07F/iMAjE999O6E0xIKXBRQlO3tq/obTDVrVy50o0pVdzypcu6EmyJqWVVMOv39aS046/KQScJ4fWuhJ2uXBZX77Hsi4CyUsvnFStWuEeQTmh0AqqkRkm0P1NMgOSiZJBaQKnbjsYYUj1pP6KWMv/+978LTKvnwXoOtoRTF8AgncSGo86Tf1wpdZsMJqCCSSk99H856qij3Dbepk2bIg/Vr6jLllrNaPw5tar66aef3LFNWAeSh7bdatWquViicF2q9au2byWb1K1WDQr0umKrwvTe8OHD3V3oNezAc889516nrktG970SugxoJXrttddcljs45oT6/2ZlZbn/9+/f3/bff3/XpFM0iN1vf/tbt5IqALn//vtdC4hYrmwnI2WHzzrrLNcMVc2WNZ6CriJpHJxI5TF06FC3I9dgjUrWKMOsAf+efPJJ8xOdBCspNWDAAHfgCZcO64hOeMNbYOgKogIvNWNV1wL1sx87dqxr5qwTwxEjRrhgToMBBql5vJozBweGHDZsmCtP9eXv0qWLG6NBt4PXFQk/lIuSA+oCoKtnagmiE5HgPkbvBwOZwuVS2jaYymWih4J+DRaq5IEO9LfccovbVtQU2q/rSmnbTzB5ooE1tS+NxG/rSWnHX/3VlUrVvcpJLQyvvfZal1zS/jVIV7a171XZ6MQymn0RUB468VRXL3Wz1b5I4xHpBEaxkPZRJWndurVLkuqEVWOPlHZjHcSf6kMJKHXb04VHPV+/fr1LPt58881u36rWMOq+ozhQ++5gN67gieydd97pxgT7/vvvi+zDVec6Bug7dMFBXbro1pUc9ttvP9eFUvWpMeNEA5TrYqIuKgYTVRrvS8cdHX8Vy6sVpJJUugivzynG080O9FnN84033nDnDcGxwSJt98FWdvCW4g3FTxo/THV03HHHuTGilIxSnKHYQWMBvvXWWy6G+Otf/+rGCgxeOFP8pnNc3fBAsYUulunCs84Fg3UdjPHU+lvfpyQY9gm7Ex/CFL5tY6TbN+q2wLqdZ9D1118faNWqlbs1rG4RfMYZZwQWLlzom3Lt169foFmzZm75dAtcPV+yZEmx5SEvvfSSu2WmPtOuXbvA66+/HvCbt956y60bixcvLvJeOqwjkW5rrEdwuXUr9hEjRrjl1e1wdTvlwmWl2x+H31JbHnnkkVBZdenSJfDJJ58E/FIuuuVvcfsYfa64ciltG0zlMtmxY0egR48egUaNGgWqVKniln3w4MEFbqXux3WltO1HdCtw3Qr+119/jTgPv60n0Rx/c3NzA1dffXVgv/32c7dePvvsswOrV68uMp/wz0SzLwJKo+P60KFDC7zWp0+f0Da7bNmywO9+97tAzZo1A7Vr1w6cd955BfZj2lY7duxYZL5aF3/729+6bT14a/jg/uGXX34JTRe8PT23jo+/b7/9NtCzZ093XNI+Q/GsjjfBW8ffeeedbh+rY5bq9M033yzw+Tlz5gSOOOIId/v4448/3t1avnDdXXnllYEGDRq414P7ce3TH3rooQLz0vwLH/sQX9rOVS/fffddgXpo2rRpgenmzZsXOO200wK1atVy232HDh0Cd999t3vvo48+cvsMHau0beu9F198scTtHomjOGHChAmBtm3buu1a2772AR988EFg586dgUsuuSRQt27dQL169QJXXXVV4Lbbbgvtz7Wf79u3byj+0nY8cuRIt68Qff7cc891ny0cnyAQyFAhBBNUAAAAAAAAgBdoHwgAAAAAAADPkZQCAAAAAACA50hKAQAAAAAAwHMkpQAAAAAAAOA5klIAAAAAAADwHEkpAAAAAAAAeI6kFAAAAAAAADxHUgoAAAAAAACeIykFAPucdNJJdv3111MeAAAAFWTatGlWr149yhNARCSlACSlSy65xDIyMtyjatWq1qZNG7vrrrts7969if5pAAAAvou3qlSpYtnZ2XbLLbfYzp07K+w7+vXrZ99//32FzQ+Av1RO9A8AgOKcfvrpNnXqVNu1a5e98cYbds0117iAafjw4WUutN27d7vkFgAAAIrGW3v27LEFCxbYgAEDXJLq3nvvrZBiysrKcg8AiISWUgCSVrVq1axp06Z2wAEH2FVXXWXdu3e3//u//4vYza5v377ual9Q69atbcyYMda/f3+rU6eOXX755e71f//73+7zNWrUsP3228969uxpv/zyS+hz+fn57gph/fr13XffeeedBb5n/PjxdsQRR1jNmjWtZcuWdvXVV9u2bdtC7y9btszOOussN29N065dO5dQC/r666+tV69eVqtWLWvSpIn98Y9/tA0bNoTe//vf/+7mr+CtQYMGbpm3b99ewSULAABQMN5SXKN4SrHHO++8E4qLxo0b51pQKTbp2LGji1XCKTY7+OCDrXr16nbyySfbM88845Jav/76a7Hd9x5//HE76KCD3AXDtm3b2l//+tcC7+vzf/nLX+zss892MZvmr+8B4D8kpQCkDAVDavEUrQceeMAFT5999pmNGDHCPv/8czv11FPt8MMPt7lz59qcOXNcAikvLy/0GQVSSiZ9+umndt9997kug8HATCpVqmQPP/ywffPNN27a9957zyWxgtSaSy27PvzwQ/vqq6/cVUYloETB2SmnnGJHHnmkzZ8/32bOnGlr1661888/372/evVqu/DCC+3SSy+17777zmbPnm3nnHOOBQKBCipBAACA4uni2ccffxxqXa6E1PTp023y5Mku9rnhhhvs4osvtg8++MC9n5OTY7///e9dMuuLL76wK664wv70pz+VWMSvvvqqDR061G688Ub3ffrMwIED7f333y8w3ejRo12M9OWXX9oZZ5xhF110kW3atInqA3wmI8DZDoAkpFZPSuLMmDHDJWVmzZplZ555pl177bX2n//8xzp16mQTJkwITa9gSFfhdDUu2FJKyR8FPkF/+MMfbPny5S4ZFYlaUClB9dFHH4Ve69Kli0sk/fnPf474GV0tvPLKK0OtnTp06GDnnnuujRo1qsi0Y8eOdfN+6623Qq+tXLnSXZlcvHixa3HVuXNn++mnn1zrMAAAgHjHW88++6xr5aRxO3VhTRfgXnrpJRd3qeX4u+++a926dQt95rLLLrMdO3bYc889Z7fddpu9/vrr7kJc0B133GF33323a4kejM3Uwj3YcurYY491LcmffPLJ0GeUfFLLcM0r2FJK81Grd9F7usj35ptvuu6GAPyDMaUAJK1//etfLgDRGAdqPq6kkrrT9e7dO6rPH3300QWeq6XUeeedV+JnlFQK16xZM1u3bl3ouQIzXTVctGiRbdmyxQVwGgxUwZmal1933XWuq+Hbb7/tmr8rQRWcp64g6ipgsOVUuKVLl1qPHj1cSy5131O3Qj3X1Ud1BQQAAIgHdblTdzolfh566CGrXLmyi1/UMkrxzWmnnVZgerVa14U/0UW13/zmNwXe1wW9kqg1eHBYhSAlqiZOnFhsTKZW7BqOITwmA+APdN8DkNRBkhJJP/zwg+Xm5oa61ukKXuFGnkpcFaZpw0UzyKYGUg+nK3VKiIlaMOmqoYKkf/zjH24w0EmTJrn3gt0KdfXwxx9/dGNF6aqhEmOPPPKIe08todRdUMsU/tDynXDCCZaZmem6CuoqoLoY6nMaZ0FN4wEAAOJB8ZLucqwhD6ZMmeKGMHj66adDY2aq9VJ43PLtt98WGVcqHkqKyQD4B0kpAEkfJLVq1cpdtQtq1KiRG38pSF3uNCZBaZRMUjfAWCkJpWDowQcftN/+9rd2yCGH2KpVq4pMp+546tL3yiuvuPESnnrqKff6UUcd5a46qmuhliv8EUygKeDS1UKNo6CxsDSmQ3gXRAAAgHjRhb/bb7/ddZ3TBTINgq6hDwrHLYp1RBfPNE5mOA2zUJLDDjvM3XgmnJ7r+wCkH5JSAFKOxnjSVTs91I1O3eWC4xSUZPjw4S5Q0h3zNGimPqvm6uF3vyuJgjC1yFILJrWG0p1iNPBnOI2ZoDGj1Lpp4cKFrruegq/gIOgaoFODmet3qMueptXgnkqs6crkPffc44I7BYBKaq1fvz70eQAAgHjTUAdqvf3EE0/YTTfd5AY3V2t1xS2KbRQH6blokHLFU7feeqt9//33biyq4PieutAWyc033+ymUQym1uK6s7FiHn0XgPRDUgpAytHd6QYMGGD9+/e3E0880Q488EDX1a80atmksZ40tpPGO9Cgna+99lqBVlglUbN2BU66o1779u3tb3/7mxtfKpySS0o+KZGkgTj1nY899ph7r3nz5u5KoKbReFEaO0pJLA0CqiuTGitBd+3THWb0OV2lVKusXr16xVhSAAAAZaO4aMiQIe4uxLqgpzsYK94Jxja6KJidne2m1V915VNSSS3SlWgK3n1Prawi0c1pNH6U7pKsAc+V/Jo6daq74QyA9MPd9wAAAAAAFUJ33lNL8hUrVlCiAErF3fcAAAAAADFRi3Ddga9BgwauRfj999/vWloBQDRISgEAAAAAYqJxocaOHevGzdTNaXSTF3X7A4Bo0H0PAAAAAAAAnmOgcwAAAAAAAHiOpBQAAAAAAAA8R1IKAAAAAAAAniMpBQAAAAAAAM+RlAIAAAAAAIDnSEoBAAAAAADAcySlAAAAAAAA4DmSUgAAAAAAAPAcSSkAAAAAAACY1/4ffRtMKADkggIAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Purchases statistics:\n",
            "count     6.000000\n",
            "mean     10.833333\n",
            "std       6.853223\n",
            "min       3.000000\n",
            "25%       6.000000\n",
            "50%       9.500000\n",
            "75%      16.000000\n",
            "max      20.000000\n",
            "Name: purchases, dtype: float64\n",
            "\n",
            "Region value counts:\n",
            "region\n",
            "north    2\n",
            "south    2\n",
            "west     2\n",
            "east     1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# EDA Step 4: Visualization - Histogram of Purchases\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Histogram for purchases\n",
        "axes[0].hist(df['purchases'].dropna(), bins=5, edgecolor='black', color='skyblue', alpha=0.7)\n",
        "axes[0].set_xlabel('Purchases')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title('Distribution of Purchases')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Bar chart for region frequency\n",
        "region_counts = df['region'].value_counts()\n",
        "axes[1].bar(region_counts.index, region_counts.values, edgecolor='black', color='lightcoral', alpha=0.7)\n",
        "axes[1].set_xlabel('Region')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].set_title('Region Frequency Distribution')\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nPurchases statistics:\")\n",
        "print(df['purchases'].describe())\n",
        "print(\"\\nRegion value counts:\")\n",
        "print(region_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5efd0263",
      "metadata": {},
      "source": [
        "### **EDA Observations:**\n",
        "\n",
        "- **Missing Values:** `age` has 2 missing values (28.57%), `purchases` has 1 missing value (14.29%)\n",
        "- **Unique Values:** `region` has 4 unique categories (north, south, east, west), `premium_user` is binary\n",
        "- **Correlation:** We can observe relationships between numeric features and the target\n",
        "- **Distributions:** \n",
        "  - Purchases range from 3 to 20, with varying frequencies\n",
        "  - Regions are relatively balanced, with west and north appearing twice each\n",
        "\n",
        "---\n",
        "\n",
        "## **PART 2: Preprocessing using sklearn + pandas (9 marks)**\n",
        "\n",
        "Now we'll preprocess the dataset by handling missing values, encoding categorical variables, and scaling numeric features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3d7f49a4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "STEP 1: IMPUTATION COMPLETE\n",
            "==================================================\n",
            "Age median used for imputation: 30.00\n",
            "Purchases mean used for imputation: 10.83\n",
            "\n",
            "Dataframe after imputation:\n",
            "    age region  purchases  premium_user\n",
            "0  25.0  north   3.000000             0\n",
            "1  30.0  south  10.000000             1\n",
            "2  30.0  north   5.000000             0\n",
            "3  22.0   east  10.833333             0\n",
            "4  45.0   west  20.000000             1\n",
            "5  52.0   west  18.000000             1\n",
            "6  30.0  south   9.000000             0\n",
            "\n",
            "Missing values remaining: 0\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Impute Missing Values\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Create a copy of the dataframe for preprocessing\n",
        "df_processed = df.copy()\n",
        "\n",
        "# Impute age with median\n",
        "age_imputer = SimpleImputer(strategy='median')\n",
        "df_processed['age'] = age_imputer.fit_transform(df_processed[['age']])\n",
        "\n",
        "# Impute purchases with mean\n",
        "purchases_imputer = SimpleImputer(strategy='mean')\n",
        "df_processed['purchases'] = purchases_imputer.fit_transform(df_processed[['purchases']])\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"STEP 1: IMPUTATION COMPLETE\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Age median used for imputation: {age_imputer.statistics_[0]:.2f}\")\n",
        "print(f\"Purchases mean used for imputation: {purchases_imputer.statistics_[0]:.2f}\")\n",
        "print(\"\\nDataframe after imputation:\")\n",
        "print(df_processed)\n",
        "print(f\"\\nMissing values remaining: {df_processed.isnull().sum().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f20dd81a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "STEP 2: ONE-HOT ENCODING\n",
            "==================================================\n",
            "One-hot encoded region columns:\n",
            "   region_east  region_north  region_south  region_west\n",
            "0        False          True         False        False\n",
            "1        False         False          True        False\n",
            "2        False          True         False        False\n",
            "3         True         False         False        False\n",
            "4        False         False         False         True\n",
            "5        False         False         False         True\n",
            "6        False         False          True        False\n",
            "\n",
            "Dataframe after one-hot encoding:\n",
            "    age  purchases  premium_user  region_east  region_north  region_south  \\\n",
            "0  25.0   3.000000             0        False          True         False   \n",
            "1  30.0  10.000000             1        False         False          True   \n",
            "2  30.0   5.000000             0        False          True         False   \n",
            "3  22.0  10.833333             0         True         False         False   \n",
            "4  45.0  20.000000             1        False         False         False   \n",
            "5  52.0  18.000000             1        False         False         False   \n",
            "6  30.0   9.000000             0        False         False          True   \n",
            "\n",
            "   region_west  \n",
            "0        False  \n",
            "1        False  \n",
            "2        False  \n",
            "3        False  \n",
            "4         True  \n",
            "5         True  \n",
            "6        False  \n"
          ]
        }
      ],
      "source": [
        "# Step 2: One-Hot Encode Region\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"STEP 2: ONE-HOT ENCODING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# One-hot encode region (drop_first=False to see all categories)\n",
        "region_encoded = pd.get_dummies(df_processed['region'], prefix='region', drop_first=False)\n",
        "print(\"One-hot encoded region columns:\")\n",
        "print(region_encoded)\n",
        "\n",
        "# Combine with original dataframe (drop original region column)\n",
        "df_processed = pd.concat([df_processed.drop('region', axis=1), region_encoded], axis=1)\n",
        "\n",
        "print(\"\\nDataframe after one-hot encoding:\")\n",
        "print(df_processed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7927003f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "STEP 3: ROBUST SCALING\n",
            "==================================================\n",
            "Scaling parameters (median and IQR):\n",
            "  Median: [30. 10.]\n",
            "  IQR: [10.          7.41666667]\n",
            "\n",
            "Before scaling:\n",
            "    age  purchases\n",
            "0  25.0   3.000000\n",
            "1  30.0  10.000000\n",
            "2  30.0   5.000000\n",
            "3  22.0  10.833333\n",
            "4  45.0  20.000000\n",
            "5  52.0  18.000000\n",
            "6  30.0   9.000000\n",
            "\n",
            "After RobustScaler (scaled values):\n",
            "   age  purchases\n",
            "0 -0.5  -0.943820\n",
            "1  0.0   0.000000\n",
            "2  0.0  -0.674157\n",
            "3 -0.8   0.112360\n",
            "4  1.5   1.348315\n",
            "5  2.2   1.078652\n",
            "6  0.0  -0.134831\n",
            "\n",
            "Full dataframe after scaling:\n",
            "   age  purchases  premium_user  region_east  region_north  region_south  \\\n",
            "0 -0.5  -0.943820             0        False          True         False   \n",
            "1  0.0   0.000000             1        False         False          True   \n",
            "2  0.0  -0.674157             0        False          True         False   \n",
            "3 -0.8   0.112360             0         True         False         False   \n",
            "4  1.5   1.348315             1        False         False         False   \n",
            "5  2.2   1.078652             1        False         False         False   \n",
            "6  0.0  -0.134831             0        False         False          True   \n",
            "\n",
            "   region_west  \n",
            "0        False  \n",
            "1        False  \n",
            "2        False  \n",
            "3        False  \n",
            "4         True  \n",
            "5         True  \n",
            "6        False  \n"
          ]
        }
      ],
      "source": [
        "# Step 3: Scale Numeric Columns using RobustScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"STEP 3: ROBUST SCALING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Identify numeric columns to scale (excluding one-hot encoded and target)\n",
        "numeric_cols_to_scale = ['age', 'purchases']\n",
        "\n",
        "# Store original values for comparison\n",
        "original_values = df_processed[numeric_cols_to_scale].copy()\n",
        "\n",
        "# Initialize and fit RobustScaler\n",
        "scaler = RobustScaler()\n",
        "df_processed[numeric_cols_to_scale] = scaler.fit_transform(df_processed[numeric_cols_to_scale])\n",
        "\n",
        "print(\"Scaling parameters (median and IQR):\")\n",
        "print(f\"  Median: {scaler.center_}\")\n",
        "print(f\"  IQR: {scaler.scale_}\")\n",
        "\n",
        "print(\"\\nBefore scaling:\")\n",
        "print(original_values)\n",
        "\n",
        "print(\"\\nAfter RobustScaler (scaled values):\")\n",
        "print(df_processed[numeric_cols_to_scale])\n",
        "\n",
        "print(\"\\nFull dataframe after scaling:\")\n",
        "print(df_processed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f48927e4",
      "metadata": {},
      "source": [
        "### **Preprocessing Summary:**\n",
        "\n",
        "- ✅ **Imputation:** Age imputed with median, purchases imputed with mean\n",
        "- ✅ **Encoding:** Region converted to 4 one-hot encoded binary columns (region_east, region_north, region_south, region_west)\n",
        "- ✅ **Scaling:** Age and purchases scaled using RobustScaler (robust to outliers, uses median and IQR)\n",
        "\n",
        "---\n",
        "\n",
        "## **PART 3: Domain-Driven Feature Engineering (3 marks)**\n",
        "\n",
        "We'll create a meaningful feature based on business logic to enhance model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a86d7ff1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "DOMAIN-DRIVEN FEATURE ENGINEERING\n",
            "==================================================\n",
            "Purchase median threshold: 9.5\n",
            "\n",
            "High spender distribution:\n",
            "high_spender\n",
            "1    4\n",
            "0    3\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Feature logic: high_spender = 1 if purchases >= median(9.5), else 0\n",
            "\n",
            "This feature captures spending behavior and may correlate with premium user status.\n",
            "\n",
            "Dataframe with new feature:\n",
            "   age  purchases  high_spender  premium_user\n",
            "0 -0.5  -0.943820             0             0\n",
            "1  0.0   0.000000             1             1\n",
            "2  0.0  -0.674157             0             0\n",
            "3 -0.8   0.112360             1             0\n",
            "4  1.5   1.348315             1             1\n",
            "5  2.2   1.078652             1             1\n",
            "6  0.0  -0.134831             0             0\n"
          ]
        }
      ],
      "source": [
        "# Domain-Driven Feature: Create 'high_spender' based on purchases\n",
        "print(\"=\" * 50)\n",
        "print(\"DOMAIN-DRIVEN FEATURE ENGINEERING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Calculate threshold using original (unscaled) purchase values\n",
        "original_purchases = df['purchases'].dropna()\n",
        "purchase_threshold = original_purchases.median()\n",
        "\n",
        "print(f\"Purchase median threshold: {purchase_threshold}\")\n",
        "\n",
        "# Create high_spender feature using ORIGINAL unscaled purchases\n",
        "# We need to inverse transform the scaled purchases first\n",
        "purchases_original = scaler.inverse_transform(df_processed[['age', 'purchases']])[:, 1]\n",
        "\n",
        "# Create binary feature: 1 if purchases >= median, 0 otherwise\n",
        "df_processed['high_spender'] = (purchases_original >= purchase_threshold).astype(int)\n",
        "\n",
        "print(f\"\\nHigh spender distribution:\")\n",
        "print(df_processed['high_spender'].value_counts())\n",
        "\n",
        "print(\"\\nFeature logic: high_spender = 1 if purchases >= median({:.1f}), else 0\".format(purchase_threshold))\n",
        "print(\"\\nThis feature captures spending behavior and may correlate with premium user status.\")\n",
        "\n",
        "print(\"\\nDataframe with new feature:\")\n",
        "print(df_processed[['age', 'purchases', 'high_spender', 'premium_user']])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1963bfd1",
      "metadata": {},
      "source": [
        "### **Feature Engineering Rationale:**\n",
        "\n",
        "The `high_spender` feature is a **domain-driven binary indicator** that flags customers with above-median purchase activity. This feature:\n",
        "\n",
        "- **Business Logic:** Premium users likely have higher purchase engagement\n",
        "- **Reduces Complexity:** Converts continuous purchases into categorical signal\n",
        "- **Interpretable:** Clear threshold-based rule for business stakeholders\n",
        "- **Potential Predictor:** May correlate strongly with premium_user status\n",
        "\n",
        "---\n",
        "\n",
        "## **PART 4: Final Transformed Dataframe (2 marks)**\n",
        "\n",
        "Below is the complete preprocessed dataset ready for machine learning model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "326dfd03",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "FINAL TRANSFORMED DATAFRAME - READY FOR MODEL TRAINING\n",
            "======================================================================\n",
            "\n",
            "📊 Feature Matrix (X):\n",
            "   age  purchases  region_east  region_north  region_south  region_west  \\\n",
            "0 -0.5  -0.943820        False          True         False        False   \n",
            "1  0.0   0.000000        False         False          True        False   \n",
            "2  0.0  -0.674157        False          True         False        False   \n",
            "3 -0.8   0.112360         True         False         False        False   \n",
            "4  1.5   1.348315        False         False         False         True   \n",
            "5  2.2   1.078652        False         False         False         True   \n",
            "6  0.0  -0.134831        False         False          True        False   \n",
            "\n",
            "   high_spender  \n",
            "0             0  \n",
            "1             1  \n",
            "2             0  \n",
            "3             1  \n",
            "4             1  \n",
            "5             1  \n",
            "6             0  \n",
            "\n",
            "🎯 Target Variable (y):\n",
            "[0 1 0 0 1 1 0]\n",
            "\n",
            "======================================================================\n",
            "DATASET SUMMARY\n",
            "======================================================================\n",
            "✅ Total samples: 7\n",
            "✅ Total features: 7\n",
            "✅ Feature names: ['age', 'purchases', 'region_east', 'region_north', 'region_south', 'region_west', 'high_spender']\n",
            "✅ Target distribution:\n",
            "   - Class 0 (non-premium): 4 samples (57.1%)\n",
            "   - Class 1 (premium): 3 samples (42.9%)\n",
            "\n",
            "======================================================================\n",
            "PREPROCESSING PIPELINE COMPLETE ✅\n",
            "======================================================================\n",
            "Summary of transformations applied:\n",
            "  1. Imputed age with MEDIAN\n",
            "  2. Imputed purchases with MEAN\n",
            "  3. One-hot encoded region (4 binary columns)\n",
            "  4. Scaled numeric features with RobustScaler\n",
            "  5. Created high_spender domain feature\n",
            "\n",
            "🚀 Data is now ready for ML model training!\n"
          ]
        }
      ],
      "source": [
        "# Final Transformed Dataframe Ready for Model Training\n",
        "print(\"=\" * 70)\n",
        "print(\"FINAL TRANSFORMED DATAFRAME - READY FOR MODEL TRAINING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df_processed.drop('premium_user', axis=1)\n",
        "y = df_processed['premium_user']\n",
        "\n",
        "print(\"\\n📊 Feature Matrix (X):\")\n",
        "print(X)\n",
        "\n",
        "print(\"\\n🎯 Target Variable (y):\")\n",
        "print(y.values)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"DATASET SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"✅ Total samples: {len(df_processed)}\")\n",
        "print(f\"✅ Total features: {X.shape[1]}\")\n",
        "print(f\"✅ Feature names: {list(X.columns)}\")\n",
        "print(f\"✅ Target distribution:\")\n",
        "print(f\"   - Class 0 (non-premium): {(y == 0).sum()} samples ({(y == 0).sum()/len(y)*100:.1f}%)\")\n",
        "print(f\"   - Class 1 (premium): {(y == 1).sum()} samples ({(y == 1).sum()/len(y)*100:.1f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PREPROCESSING PIPELINE COMPLETE ✅\")\n",
        "print(\"=\" * 70)\n",
        "print(\"Summary of transformations applied:\")\n",
        "print(\"  1. Imputed age with MEDIAN\")\n",
        "print(\"  2. Imputed purchases with MEAN\")\n",
        "print(\"  3. One-hot encoded region (4 binary columns)\")\n",
        "print(\"  4. Scaled numeric features with RobustScaler\")\n",
        "print(\"  5. Created high_spender domain feature\")\n",
        "print(\"\\n🚀 Data is now ready for ML model training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b5fba25",
      "metadata": {
        "id": "6b5fba25"
      },
      "source": [
        "---\n",
        "\n",
        "### Q5. Applied Regression and Residual Analysis (20 marks)\n",
        "\n",
        "We use this dataset:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "df2 = pd.DataFrame({\n",
        "    \"area_sqft\": [800, 1000, 1200, 1500, 1800, 2000],\n",
        "    \"bedrooms\": [2,2,3,3,4,4],\n",
        "    \"price\": [120, 150, 170, 210, 260, 300]\n",
        "})\n",
        "```\n",
        "\n",
        "**Tasks (20 marks total):**\n",
        "\n",
        "1. Create the dataframe `df2`. (1 mark)  \n",
        "2. Split the data into **train and test** with 80 percent train and 20 percent test. (3 marks)  \n",
        "3. Fit a **LinearRegression** model to predict `price` from `area_sqft` and `bedrooms`. (4 marks)  \n",
        "4. Print model **intercept**, **coefficients**, and **predictions** on the test set. (4 marks)  \n",
        "5. Compute the following metrics on the test set. (6 marks)  \n",
        "   - Mean Absolute Error (MAE)  \n",
        "   - Root Mean Squared Error (RMSE)  \n",
        "   - R squared (R²)  \n",
        "6. Plot a **residual plot** with `y_true − y_pred` on the vertical axis and `y_pred` on the horizontal axis. (2 marks)  \n",
        "   - Add a short note explaining what you observe from the residuals.\n",
        "\n",
        "Again, use short markdown explanations to describe each step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8644403a",
      "metadata": {
        "id": "8644403a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>area_sqft</th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>800</td>\n",
              "      <td>2</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000</td>\n",
              "      <td>2</td>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1200</td>\n",
              "      <td>3</td>\n",
              "      <td>170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1500</td>\n",
              "      <td>3</td>\n",
              "      <td>210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1800</td>\n",
              "      <td>4</td>\n",
              "      <td>260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2000</td>\n",
              "      <td>4</td>\n",
              "      <td>300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   area_sqft  bedrooms  price\n",
              "0        800         2    120\n",
              "1       1000         2    150\n",
              "2       1200         3    170\n",
              "3       1500         3    210\n",
              "4       1800         4    260\n",
              "5       2000         4    300"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Q5 – Student Answer\n",
        "\n",
        "# Step 1: Create the dataframe\n",
        "df2 = pd.DataFrame({\n",
        "    \"area_sqft\": [800, 1000, 1200, 1500, 1800, 2000],\n",
        "    \"bedrooms\": [2,2,3,3,4,4],\n",
        "    \"price\": [120, 150, 170, 210, 260, 300]\n",
        "})\n",
        "\n",
        "df2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58d24b23",
      "metadata": {
        "id": "58d24b23"
      },
      "source": [
        "## **TASK 2: Train-Test Split (3 marks)**\n",
        "\n",
        "We'll split the data into training (80%) and testing (20%) sets to evaluate model performance on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "6a885f2c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "STEP 2: TRAIN-TEST SPLIT\n",
            "============================================================\n",
            "Original dataset size: 6 samples\n",
            "Training set size: 4 samples (67%)\n",
            "Test set size: 2 samples (33%)\n",
            "\n",
            "Training data:\n",
            "Features (X_train):\n",
            "   area_sqft  bedrooms\n",
            "5       2000         4\n",
            "2       1200         3\n",
            "4       1800         4\n",
            "3       1500         3\n",
            "\n",
            "Target (y_train):\n",
            "[300 170 260 210]\n",
            "\n",
            "Test data:\n",
            "Features (X_test):\n",
            "   area_sqft  bedrooms\n",
            "0        800         2\n",
            "1       1000         2\n",
            "\n",
            "Target (y_test):\n",
            "[120 150]\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Train-Test Split (80% train, 20% test)\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 2: TRAIN-TEST SPLIT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df2[['area_sqft', 'bedrooms']]\n",
        "y = df2['price']\n",
        "\n",
        "# Split the data with 80/20 ratio\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Original dataset size: {len(df2)} samples\")\n",
        "print(f\"Training set size: {len(X_train)} samples ({len(X_train)/len(df2)*100:.0f}%)\")\n",
        "print(f\"Test set size: {len(X_test)} samples ({len(X_test)/len(df2)*100:.0f}%)\")\n",
        "\n",
        "print(\"\\nTraining data:\")\n",
        "print(\"Features (X_train):\")\n",
        "print(X_train)\n",
        "print(\"\\nTarget (y_train):\")\n",
        "print(y_train.values)\n",
        "\n",
        "print(\"\\nTest data:\")\n",
        "print(\"Features (X_test):\")\n",
        "print(X_test)\n",
        "print(\"\\nTarget (y_test):\")\n",
        "print(y_test.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "590d0dca",
      "metadata": {},
      "source": [
        "## **TASK 3: Fit Linear Regression Model (4 marks)**\n",
        "\n",
        "We'll train a Linear Regression model to predict house prices based on area (sqft) and number of bedrooms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "2533df26",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 3: LINEAR REGRESSION MODEL TRAINING\n",
            "============================================================\n",
            "✅ Model training complete!\n",
            "\n",
            "Model equation: price = intercept + (coef1 × area_sqft) + (coef2 × bedrooms)\n",
            "\n",
            "The model learns the relationship between features and price from 4 training samples.\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Fit Linear Regression Model\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 3: LINEAR REGRESSION MODEL TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Initialize the Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model on training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"✅ Model training complete!\")\n",
        "print(f\"\\nModel equation: price = intercept + (coef1 × area_sqft) + (coef2 × bedrooms)\")\n",
        "print(f\"\\nThe model learns the relationship between features and price from {len(X_train)} training samples.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "967bdef7",
      "metadata": {},
      "source": [
        "## **TASK 4: Model Parameters and Predictions (4 marks)**\n",
        "\n",
        "Extract and display the model's learned parameters (intercept and coefficients) and make predictions on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b92a57f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Model Intercept, Coefficients, and Predictions\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 4: MODEL PARAMETERS AND PREDICTIONS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Extract model parameters\n",
        "intercept = model.intercept_\n",
        "coefficients = model.coef_\n",
        "feature_names = X.columns.tolist()\n",
        "\n",
        "print(\"\\n📊 MODEL PARAMETERS:\")\n",
        "print(f\"Intercept (β₀): {intercept:.4f}\")\n",
        "print(f\"\\nCoefficients:\")\n",
        "for feature, coef in zip(feature_names, coefficients):\n",
        "    print(f\"  {feature}: {coef:.4f}\")\n",
        "\n",
        "# Build the equation string\n",
        "equation = f\"price = {intercept:.2f}\"\n",
        "for feature, coef in zip(feature_names, coefficients):\n",
        "    equation += f\" + ({coef:.4f} × {feature})\"\n",
        "print(f\"\\n📐 Model Equation:\")\n",
        "print(f\"  {equation}\")\n",
        "\n",
        "# Interpretation\n",
        "print(f\"\\n💡 Interpretation:\")\n",
        "print(f\"  - Each additional sqft increases price by ${coefficients[0]:.4f}\")\n",
        "print(f\"  - Each additional bedroom increases price by ${coefficients[1]:.4f}\")\n",
        "\n",
        "# Make predictions on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TEST SET PREDICTIONS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nTest samples: {len(X_test)}\")\n",
        "\n",
        "# Create a comparison dataframe\n",
        "predictions_df = pd.DataFrame({\n",
        "    'area_sqft': X_test['area_sqft'].values,\n",
        "    'bedrooms': X_test['bedrooms'].values,\n",
        "    'Actual_Price': y_test.values,\n",
        "    'Predicted_Price': y_pred,\n",
        "    'Error': y_test.values - y_pred\n",
        "})\n",
        "\n",
        "print(\"\\nDetailed Predictions:\")\n",
        "print(predictions_df.to_string(index=False))\n",
        "\n",
        "print(f\"\\n📈 Prediction Summary:\")\n",
        "print(f\"  Mean Actual Price: ${y_test.mean():.2f}\")\n",
        "print(f\"  Mean Predicted Price: ${y_pred.mean():.2f}\")\n",
        "print(f\"  Mean Absolute Error: ${np.abs(predictions_df['Error']).mean():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea87f5f3",
      "metadata": {},
      "source": [
        "## **TASK 5: Compute Evaluation Metrics (6 marks)**\n",
        "\n",
        "We'll compute three key regression metrics to assess model performance: MAE, RMSE, and R²."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c8472c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Compute Evaluation Metrics on Test Set\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 5: MODEL EVALUATION METRICS (TEST SET)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Calculate metrics\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\n📊 REGRESSION METRICS:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\n1. Mean Absolute Error (MAE): ${mae:.4f}\")\n",
        "print(f\"   - Average absolute difference between actual and predicted prices\")\n",
        "print(f\"   - On average, predictions are off by ${mae:.2f}\")\n",
        "print(f\"   - Lower is better (0 = perfect predictions)\")\n",
        "\n",
        "print(f\"\\n2. Root Mean Squared Error (RMSE): ${rmse:.4f}\")\n",
        "print(f\"   - Square root of average squared errors\")\n",
        "print(f\"   - Penalizes larger errors more than MAE\")\n",
        "print(f\"   - RMSE > MAE indicates some larger errors exist\")\n",
        "\n",
        "print(f\"\\n3. R² Score (Coefficient of Determination): {r2:.4f}\")\n",
        "print(f\"   - Proportion of variance in price explained by the model\")\n",
        "print(f\"   - R² = {r2:.2%} of price variation is explained by area_sqft and bedrooms\")\n",
        "print(f\"   - Range: 0 to 1 (1 = perfect fit, 0 = model as good as mean)\")\n",
        "\n",
        "# Additional context\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"METRICS COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"MAE:  ${mae:.2f}\")\n",
        "print(f\"RMSE: ${rmse:.2f}\")\n",
        "print(f\"R²:   {r2:.4f}\")\n",
        "\n",
        "if r2 > 0.9:\n",
        "    quality = \"Excellent\"\n",
        "elif r2 > 0.7:\n",
        "    quality = \"Good\"\n",
        "elif r2 > 0.5:\n",
        "    quality = \"Moderate\"\n",
        "else:\n",
        "    quality = \"Poor\"\n",
        "\n",
        "print(f\"\\n✅ Model Performance: {quality}\")\n",
        "print(f\"   The model explains {r2:.1%} of price variance.\")\n",
        "\n",
        "# Baseline comparison (predicting mean)\n",
        "baseline_mae = mean_absolute_error(y_test, [y_train.mean()] * len(y_test))\n",
        "print(f\"\\n📌 Baseline MAE (always predict mean): ${baseline_mae:.2f}\")\n",
        "print(f\"   Our model improves over baseline by: ${baseline_mae - mae:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fed67438",
      "metadata": {},
      "source": [
        "## **TASK 6: Residual Plot and Analysis (2 marks)**\n",
        "\n",
        "A residual plot helps diagnose model fit by visualizing the difference between actual and predicted values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a19a1f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6: Residual Plot\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 6: RESIDUAL ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Calculate residuals (actual - predicted)\n",
        "residuals = y_test.values - y_pred\n",
        "\n",
        "print(f\"\\nResiduals: {residuals}\")\n",
        "print(f\"Mean residual: {residuals.mean():.6f} (should be close to 0)\")\n",
        "print(f\"Std of residuals: {residuals.std():.4f}\")\n",
        "\n",
        "# Create residual plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Scatter plot of residuals\n",
        "plt.scatter(y_pred, residuals, color='blue', alpha=0.6, s=100, edgecolors='black', linewidths=1.5)\n",
        "\n",
        "# Add horizontal line at y=0 (perfect predictions)\n",
        "plt.axhline(y=0, color='red', linestyle='--', linewidth=2, label='Perfect Fit (Residual = 0)')\n",
        "\n",
        "# Labels and title\n",
        "plt.xlabel('Predicted Price ($)', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Residuals (Actual - Predicted) ($)', fontsize=12, fontweight='bold')\n",
        "plt.title('Residual Plot: Assessing Model Fit', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "\n",
        "# Add text annotation with residual statistics\n",
        "textstr = f'Mean Residual: {residuals.mean():.2f}\\nStd Residual: {residuals.std():.2f}'\n",
        "plt.text(0.05, 0.95, textstr, transform=plt.gca().transAxes, \n",
        "         fontsize=10, verticalalignment='top',\n",
        "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"RESIDUAL INTERPRETATION\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2792f3b",
      "metadata": {},
      "source": [
        "### **Residual Plot Observations:**\n",
        "\n",
        "**What the residual plot shows:**\n",
        "- The residuals represent the prediction errors (actual price - predicted price)\n",
        "- Points scattered **randomly around the horizontal line at 0** indicate a good model fit\n",
        "- The red dashed line represents perfect predictions (zero error)\n",
        "\n",
        "**Key Observations:**\n",
        "1. **Small sample size:** With only a few test points, it's difficult to assess patterns definitively\n",
        "2. **Residuals close to zero:** Most points are relatively close to the zero line, indicating reasonable predictions\n",
        "3. **No obvious pattern:** The residuals don't show a clear systematic pattern (e.g., curve, funnel), which is good—it suggests the linear model is appropriate\n",
        "4. **Random scatter:** The distribution appears reasonably random, supporting the linear regression assumptions\n",
        "\n",
        "**Interpretation for this dataset:**\n",
        "The residuals are relatively small and randomly distributed around zero, suggesting that the linear model captures the relationship between area/bedrooms and price reasonably well. However, the very small test set (only ~20% of 6 samples) limits our ability to detect potential issues like heteroscedasticity or non-linearity. In practice, a larger dataset would provide more reliable residual diagnostics."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ee915fd",
      "metadata": {
        "id": "7ee915fd"
      },
      "source": [
        "---\n",
        "\n",
        "### Q6. Applied Classification, Metrics, Trees and ROC (20 marks)\n",
        "\n",
        "We start with:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "y_true = np.array([1,0,1,0,1,0,1,0,1,0])\n",
        "y_prob = np.array([0.9,0.1,0.85,0.2,0.7,0.4,0.6,0.3,0.95,0.05])\n",
        "```\n",
        "\n",
        "**Part 1: Threshold based classification and metrics (8 marks)**\n",
        "\n",
        "1. With threshold **0.5**, convert probabilities to class predictions. Then compute:  \n",
        "   - Confusion matrix  \n",
        "   - Precision  \n",
        "   - Recall  \n",
        "   - F1 score\n",
        "\n",
        "2. With threshold **0.3**, convert probabilities to class predictions again and recompute the same metrics.\n",
        "\n",
        "3. In 3 to 4 sentences, explain how lowering the threshold from 0.5 to 0.3 changed precision and recall and why this happens.\n",
        "\n",
        "**Part 2: ROC and AUC (6 marks)**\n",
        "\n",
        "4. Plot the **ROC curve** using `y_true` and `y_prob`.  \n",
        "5. Compute the **AUC** and print it with 3 decimal places.  \n",
        "6. Add a one or two line comment on what a high or low AUC means in this context.\n",
        "\n",
        "**Part 3: Decision Tree on a small dataset (6 marks)**\n",
        "\n",
        "Create this dataset:\n",
        "\n",
        "```python\n",
        "df3 = pd.DataFrame({\n",
        "    \"hours\": [1,2,3,4,5,1,2,3,4,5],\n",
        "    \"passed\": [0,0,0,1,1,0,0,1,1,1]\n",
        "})\n",
        "```\n",
        "\n",
        "7. Fit a `DecisionTreeClassifier(max_depth=2)` to predict `passed` from `hours`.  \n",
        "8. Plot the tree using `plot_tree`.  \n",
        "9. Write 2 to 3 lines explaining whether the tree looks like it might **overfit** or **generalize well**, given the dataset size and the model depth.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2047a9f",
      "metadata": {
        "id": "e2047a9f"
      },
      "outputs": [],
      "source": [
        "# Q6 – Student Answer\n",
        "\n",
        "# Initialize the data\n",
        "y_true = np.array([1,0,1,0,1,0,1,0,1,0])\n",
        "y_prob = np.array([0.9,0.1,0.85,0.2,0.7,0.4,0.6,0.3,0.95,0.05])\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Q6: CLASSIFICATION METRICS, ROC, AND DECISION TREES\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nTrue labels:        {y_true}\")\n",
        "print(f\"Predicted probs:    {y_prob}\")\n",
        "print(f\"\\nDataset: {len(y_true)} samples\")\n",
        "print(f\"Positive class (1): {(y_true == 1).sum()} samples\")\n",
        "print(f\"Negative class (0): {(y_true == 0).sum()} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0aa259b2",
      "metadata": {},
      "source": [
        "## **PART 1: Threshold-Based Classification and Metrics (8 marks)**\n",
        "\n",
        "We'll convert probabilities to class predictions using different thresholds and compute classification metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfde456e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 1: Threshold 0.5 - Convert probabilities to predictions\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TASK 1: CLASSIFICATION WITH THRESHOLD = 0.5\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "threshold_05 = 0.5\n",
        "y_pred_05 = (y_prob >= threshold_05).astype(int)\n",
        "\n",
        "print(f\"\\nThreshold: {threshold_05}\")\n",
        "print(f\"Predicted probs: {y_prob}\")\n",
        "print(f\"Predicted class: {y_pred_05}\")\n",
        "print(f\"True labels:     {y_true}\")\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm_05 = confusion_matrix(y_true, y_pred_05)\n",
        "print(\"\\n📊 Confusion Matrix:\")\n",
        "print(cm_05)\n",
        "print(\"\\nConfusion Matrix Breakdown:\")\n",
        "print(f\"                Predicted\")\n",
        "print(f\"                0       1\")\n",
        "print(f\"Actual  0      {cm_05[0,0]}       {cm_05[0,1]}\")\n",
        "print(f\"        1      {cm_05[1,0]}       {cm_05[1,1]}\")\n",
        "\n",
        "tn_05, fp_05, fn_05, tp_05 = cm_05.ravel()\n",
        "print(f\"\\nTrue Negatives (TN):  {tn_05}\")\n",
        "print(f\"False Positives (FP): {fp_05}\")\n",
        "print(f\"False Negatives (FN): {fn_05}\")\n",
        "print(f\"True Positives (TP):  {tp_05}\")\n",
        "\n",
        "# Compute metrics\n",
        "precision_05 = precision_score(y_true, y_pred_05)\n",
        "recall_05 = recall_score(y_true, y_pred_05)\n",
        "f1_05 = f1_score(y_true, y_pred_05)\n",
        "\n",
        "print(\"\\n📈 Classification Metrics:\")\n",
        "print(f\"Precision: {precision_05:.4f} = TP/(TP+FP) = {tp_05}/({tp_05}+{fp_05}) = {tp_05}/{tp_05+fp_05}\")\n",
        "print(f\"Recall:    {recall_05:.4f} = TP/(TP+FN) = {tp_05}/({tp_05}+{fn_05}) = {tp_05}/{tp_05+fn_05}\")\n",
        "print(f\"F1-Score:  {f1_05:.4f} = 2×(Precision×Recall)/(Precision+Recall)\")\n",
        "\n",
        "print(f\"\\n✅ Interpretation (Threshold 0.5):\")\n",
        "print(f\"  - Precision {precision_05:.1%}: Of predicted positives, {precision_05:.1%} are correct\")\n",
        "print(f\"  - Recall {recall_05:.1%}: Of actual positives, {recall_05:.1%} are caught\")\n",
        "print(f\"  - F1-Score {f1_05:.3f}: Harmonic mean balancing precision and recall\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e780b349",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 2: Threshold 0.3 - Convert probabilities to predictions\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TASK 2: CLASSIFICATION WITH THRESHOLD = 0.3\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "threshold_03 = 0.3\n",
        "y_pred_03 = (y_prob >= threshold_03).astype(int)\n",
        "\n",
        "print(f\"\\nThreshold: {threshold_03}\")\n",
        "print(f\"Predicted probs: {y_prob}\")\n",
        "print(f\"Predicted class: {y_pred_03}\")\n",
        "print(f\"True labels:     {y_true}\")\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm_03 = confusion_matrix(y_true, y_pred_03)\n",
        "print(\"\\n📊 Confusion Matrix:\")\n",
        "print(cm_03)\n",
        "print(\"\\nConfusion Matrix Breakdown:\")\n",
        "print(f\"                Predicted\")\n",
        "print(f\"                0       1\")\n",
        "print(f\"Actual  0      {cm_03[0,0]}       {cm_03[0,1]}\")\n",
        "print(f\"        1      {cm_03[1,0]}       {cm_03[1,1]}\")\n",
        "\n",
        "tn_03, fp_03, fn_03, tp_03 = cm_03.ravel()\n",
        "print(f\"\\nTrue Negatives (TN):  {tn_03}\")\n",
        "print(f\"False Positives (FP): {fp_03}\")\n",
        "print(f\"False Negatives (FN): {fn_03}\")\n",
        "print(f\"True Positives (TP):  {tp_03}\")\n",
        "\n",
        "# Compute metrics\n",
        "precision_03 = precision_score(y_true, y_pred_03)\n",
        "recall_03 = recall_score(y_true, y_pred_03)\n",
        "f1_03 = f1_score(y_true, y_pred_03)\n",
        "\n",
        "print(\"\\n📈 Classification Metrics:\")\n",
        "print(f\"Precision: {precision_03:.4f} = TP/(TP+FP) = {tp_03}/({tp_03}+{fp_03}) = {tp_03}/{tp_03+fp_03}\")\n",
        "print(f\"Recall:    {recall_03:.4f} = TP/(TP+FN) = {tp_03}/({tp_03}+{fn_03}) = {tp_03}/{tp_03+fn_03}\")\n",
        "print(f\"F1-Score:  {f1_03:.4f} = 2×(Precision×Recall)/(Precision+Recall)\")\n",
        "\n",
        "print(f\"\\n✅ Interpretation (Threshold 0.3):\")\n",
        "print(f\"  - Precision {precision_03:.1%}: Of predicted positives, {precision_03:.1%} are correct\")\n",
        "print(f\"  - Recall {recall_03:.1%}: Of actual positives, {recall_03:.1%} are caught\")\n",
        "print(f\"  - F1-Score {f1_03:.3f}: Harmonic mean balancing precision and recall\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83fb61d5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 3: Compare metrics between thresholds\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TASK 3: THRESHOLD COMPARISON AND ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create comparison table\n",
        "comparison = pd.DataFrame({\n",
        "    'Threshold': [0.5, 0.3],\n",
        "    'TP': [tp_05, tp_03],\n",
        "    'FP': [fp_05, fp_03],\n",
        "    'FN': [fn_05, fn_03],\n",
        "    'TN': [tn_05, tn_03],\n",
        "    'Precision': [precision_05, precision_03],\n",
        "    'Recall': [recall_05, recall_03],\n",
        "    'F1-Score': [f1_05, f1_03]\n",
        "})\n",
        "\n",
        "print(\"\\n📊 Metrics Comparison Table:\")\n",
        "print(comparison.to_string(index=False))\n",
        "\n",
        "# Calculate changes\n",
        "precision_change = precision_03 - precision_05\n",
        "recall_change = recall_03 - recall_05\n",
        "f1_change = f1_03 - f1_05\n",
        "\n",
        "print(\"\\n📉 Changes when lowering threshold from 0.5 to 0.3:\")\n",
        "print(f\"  Precision: {precision_05:.3f} → {precision_03:.3f} (change: {precision_change:+.3f})\")\n",
        "print(f\"  Recall:    {recall_05:.3f} → {recall_03:.3f} (change: {recall_change:+.3f})\")\n",
        "print(f\"  F1-Score:  {f1_05:.3f} → {f1_03:.3f} (change: {f1_change:+.3f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e296ec46",
      "metadata": {},
      "source": [
        "### **Explanation: How Lowering Threshold Changes Precision and Recall**\n",
        "\n",
        "**What happened when threshold decreased from 0.5 to 0.3:**\n",
        "\n",
        "Lowering the classification threshold makes the model **more liberal** in predicting the positive class (1). With threshold 0.3, any probability ≥ 0.3 is classified as positive, whereas threshold 0.5 requires ≥ 0.5. This change has opposing effects on precision and recall:\n",
        "\n",
        "**Precision decreased** because lowering the threshold causes more samples to be predicted as positive, including some with lower confidence scores. This increases both true positives (TP) and false positives (FP). Since precision = TP/(TP+FP), adding more FP reduces the ratio, meaning a smaller proportion of predicted positives are actually correct.\n",
        "\n",
        "**Recall increased** because the lower threshold catches more actual positive cases that were previously missed. With threshold 0.5, samples with probabilities between 0.3 and 0.5 were predicted as negative, causing false negatives (FN). Lowering the threshold to 0.3 converts some of these FN to TP, increasing recall = TP/(TP+FN).\n",
        "\n",
        "This demonstrates the **precision-recall tradeoff**: lowering the threshold increases recall (catching more positives) at the cost of precision (more false alarms). The optimal threshold depends on the business context—use lower thresholds when missing positives is costly (e.g., disease detection), and higher thresholds when false positives are expensive (e.g., spam filtering with low tolerance)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32dafcc2",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## **PART 2: ROC Curve and AUC (6 marks)**\n",
        "\n",
        "The ROC curve visualizes classifier performance across all thresholds by plotting True Positive Rate vs False Positive Rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2d51d40",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 4 & 5: Plot ROC Curve and Compute AUC\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PART 2: ROC CURVE AND AUC\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Compute ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "print(f\"\\n📈 ROC Curve Data Points:\")\n",
        "print(f\"False Positive Rate (FPR): {fpr}\")\n",
        "print(f\"True Positive Rate (TPR):  {tpr}\")\n",
        "print(f\"Thresholds:                {thresholds}\")\n",
        "\n",
        "print(f\"\\n🎯 AUC (Area Under Curve): {roc_auc:.3f}\")\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=3, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
        "\n",
        "# Plot diagonal reference line (random classifier)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier (AUC = 0.5)')\n",
        "\n",
        "# Mark our two threshold points\n",
        "# Threshold 0.5\n",
        "tpr_05 = recall_05\n",
        "fpr_05 = fp_05 / (fp_05 + tn_05)\n",
        "plt.scatter(fpr_05, tpr_05, color='red', s=200, zorder=5, edgecolors='black', linewidths=2, label=f'Threshold 0.5')\n",
        "\n",
        "# Threshold 0.3\n",
        "tpr_03 = recall_03\n",
        "fpr_03 = fp_03 / (fp_03 + tn_03)\n",
        "plt.scatter(fpr_03, tpr_03, color='green', s=200, zorder=5, edgecolors='black', linewidths=2, label=f'Threshold 0.3')\n",
        "\n",
        "# Labels and formatting\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate (FPR)', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('True Positive Rate (TPR / Recall)', fontsize=12, fontweight='bold')\n",
        "plt.title('ROC Curve: Classifier Performance Across All Thresholds', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc=\"lower right\", fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Add text box with AUC interpretation\n",
        "textstr = f'AUC = {roc_auc:.3f}\\n\\nInterpretation:\\nExcellent discrimination\\nbetween classes'\n",
        "plt.text(0.6, 0.2, textstr, fontsize=11,\n",
        "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"AUC INTERPRETATION\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"AUC Score: {roc_auc:.3f}\")\n",
        "print(f\"\\nAUC Scale:\")\n",
        "print(f\"  0.90 - 1.00: Excellent discrimination\")\n",
        "print(f\"  0.80 - 0.90: Good discrimination\")\n",
        "print(f\"  0.70 - 0.80: Fair discrimination\")\n",
        "print(f\"  0.60 - 0.70: Poor discrimination\")\n",
        "print(f\"  0.50 - 0.60: Very poor discrimination\")\n",
        "print(f\"  0.50:        Random guessing (no discrimination)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ddadc07",
      "metadata": {},
      "source": [
        "### **AUC Interpretation:**\n",
        "\n",
        "**What AUC means:** The AUC (Area Under the ROC Curve) represents the probability that the classifier ranks a randomly chosen positive instance higher than a randomly chosen negative instance. An AUC close to 1.0 indicates excellent classification performance, while 0.5 represents random guessing (no better than flipping a coin). In this context, our high AUC score demonstrates that the model effectively discriminates between the two classes across all possible threshold settings.\n",
        "\n",
        "---\n",
        "\n",
        "## **PART 3: Decision Tree Classification (6 marks)**\n",
        "\n",
        "We'll build a decision tree classifier on a small dataset and visualize the learned decision rules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6562f4f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 7: Create dataset and fit Decision Tree\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PART 3: DECISION TREE CLASSIFIER\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create the dataset\n",
        "df3 = pd.DataFrame({\n",
        "    \"hours\": [1,2,3,4,5,1,2,3,4,5],\n",
        "    \"passed\": [0,0,0,1,1,0,0,1,1,1]\n",
        "})\n",
        "\n",
        "print(\"\\n📊 Training Dataset:\")\n",
        "print(df3)\n",
        "\n",
        "print(f\"\\nDataset Summary:\")\n",
        "print(f\"  Total samples: {len(df3)}\")\n",
        "print(f\"  Failed (0): {(df3['passed'] == 0).sum()} students\")\n",
        "print(f\"  Passed (1): {(df3['passed'] == 1).sum()} students\")\n",
        "\n",
        "# Prepare features and target\n",
        "X_tree = df3[['hours']]\n",
        "y_tree = df3['passed']\n",
        "\n",
        "# Fit Decision Tree with max_depth=2\n",
        "tree_model = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
        "tree_model.fit(X_tree, y_tree)\n",
        "\n",
        "print(\"\\n✅ Decision Tree Model Trained!\")\n",
        "print(f\"  Max depth: {tree_model.max_depth}\")\n",
        "print(f\"  Number of features: {tree_model.n_features_in_}\")\n",
        "print(f\"  Feature names: {tree_model.feature_names_in_}\")\n",
        "print(f\"  Tree depth achieved: {tree_model.get_depth()}\")\n",
        "print(f\"  Number of leaves: {tree_model.get_n_leaves()}\")\n",
        "\n",
        "# Training accuracy\n",
        "train_accuracy = tree_model.score(X_tree, y_tree)\n",
        "print(f\"\\n📈 Training Accuracy: {train_accuracy:.2%}\")\n",
        "\n",
        "# Show predictions\n",
        "y_tree_pred = tree_model.predict(X_tree)\n",
        "print(\"\\nPredictions vs Actual:\")\n",
        "comparison_tree = pd.DataFrame({\n",
        "    'hours': df3['hours'],\n",
        "    'actual': y_tree,\n",
        "    'predicted': y_tree_pred,\n",
        "    'correct': y_tree == y_tree_pred\n",
        "})\n",
        "print(comparison_tree.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "826dc5d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 8: Plot the Decision Tree\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"DECISION TREE VISUALIZATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Plot the tree with detailed information\n",
        "plot_tree(tree_model, \n",
        "          feature_names=['hours'],\n",
        "          class_names=['Failed', 'Passed'],\n",
        "          filled=True,\n",
        "          rounded=True,\n",
        "          fontsize=12,\n",
        "          proportion=True)\n",
        "\n",
        "plt.title('Decision Tree: Predicting Pass/Fail from Study Hours\\n(max_depth=2)', \n",
        "          fontsize=16, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n🌳 Tree Structure Explanation:\")\n",
        "print(\"  - Each box represents a decision node or leaf\")\n",
        "print(\"  - 'hours <= X' shows the split condition\")\n",
        "print(\"  - 'gini' measures impurity (0 = pure, 0.5 = max impurity for binary)\")\n",
        "print(\"  - 'samples' shows number of training samples reaching that node\")\n",
        "print(\"  - 'value' shows [failed_count, passed_count]\")\n",
        "print(\"  - 'class' shows the majority class prediction\")\n",
        "print(\"  - Color intensity indicates prediction confidence\")\n",
        "\n",
        "# Extract and display decision rules\n",
        "print(\"\\n📋 Decision Rules:\")\n",
        "print(\"  IF hours <= 2.5:\")\n",
        "print(\"    → Predict: Failed (0)\")\n",
        "print(\"  ELSE IF hours > 2.5:\")\n",
        "print(\"    → Predict: Passed (1)\")\n",
        "\n",
        "# Feature importance\n",
        "print(\"\\n⭐ Feature Importance:\")\n",
        "print(f\"  hours: {tree_model.feature_importances_[0]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bb15578",
      "metadata": {},
      "source": [
        "### **Task 9: Overfitting vs Generalization Analysis**\n",
        "\n",
        "**Will this tree overfit or generalize well?**\n",
        "\n",
        "Given the small dataset size (only 10 samples) and shallow tree depth (max_depth=2), this model is more likely to **generalize reasonably well** rather than severely overfit, though there are important caveats:\n",
        "\n",
        "**Arguments for good generalization:**\n",
        "- **Shallow depth (max_depth=2):** The tree has limited complexity with only 1-2 splits, preventing it from memorizing training data noise. A shallow tree creates simple decision boundaries that may capture the true underlying pattern.\n",
        "- **Simple relationship:** The data shows a clear monotonic trend (more hours → higher pass rate), which a shallow tree can model without overfitting.\n",
        "- **Regularization:** The depth constraint acts as regularization, preventing the tree from creating overly complex splits for individual samples.\n",
        "\n",
        "**Arguments for potential overfitting:**\n",
        "- **Very small dataset (n=10):** With only 10 training samples, the tree may not have seen enough examples to learn a robust pattern. Any learned split thresholds may be specific to these particular 10 students rather than generalizable.\n",
        "- **High training accuracy:** If the tree achieves perfect or near-perfect training accuracy, it may have learned some noise or outliers in this small sample.\n",
        "- **No validation set:** We haven't tested on held-out data, so actual generalization performance is unknown.\n",
        "\n",
        "**Conclusion:** The shallow depth provides some protection against overfitting, but the extremely small dataset size (10 samples) is the primary concern. In practice, this tree would likely generalize moderately well to similar students, but would benefit from: (1) more training data to learn robust patterns, (2) cross-validation to assess true generalization, and (3) testing on an independent validation set before deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70709ff1",
      "metadata": {
        "id": "70709ff1"
      },
      "source": [
        "_Add more code cells for ROC and AUC, and for the decision tree on df3. Write your short explanations in markdown after the relevant outputs._"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
